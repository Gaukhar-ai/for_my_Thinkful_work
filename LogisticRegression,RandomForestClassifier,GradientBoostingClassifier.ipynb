{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression,RandomForestClassifier,GradientBoostingClassifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzQvIu1kavnCzEfd2xdRRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaukhar-ai/for_my_Thinkful_work/blob/master/LogisticRegression%2CRandomForestClassifier%2CGradientBoostingClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwL8nfPymW4S"
      },
      "source": [
        "#increase the performance of the models implemented in BoW"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD7o45cEmeRV",
        "outputId": "002ef1c9-79cd-4a9d-dbe4-b19382f73e61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import gutenberg\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "#!python -m spacy download en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86kBEETXm8Ph"
      },
      "source": [
        "#utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "  #visual insp. identifies a form of punctuation spaCy doesn't recognize: the double dash '--'. better to get rid of it now.\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N22FP0z-pECY"
      },
      "source": [
        "#LOAD AND CLEAN"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maMgUAftn6h4"
      },
      "source": [
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# the Chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQ9fVW1oiEW"
      },
      "source": [
        "#PARSE AND CLEAN THE BOOKS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6wULoropOzn"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVD-4qbapO7e"
      },
      "source": [
        "#GROUP INTO SENTENCES"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAP-fDEEpO-P",
        "outputId": "2fdadda4-8c27-4aec-ad22-1e81c89959a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# combine the sentences from the two novels into one data frame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "sentences.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPRK22tQpPAu"
      },
      "source": [
        "#REMOVE STOPWORDS, PUNCTUATION, LEMMATIZE, TOKENS"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv-KwHalpPDV"
      },
      "source": [
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = \" \".join(\n",
        "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrgaNMrpPGF"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word')\n",
        "X = vectorizer.fit_transform(sentences[\"text\"])\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "sentences = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIL3g5MMpPId"
      },
      "source": [
        "#GRID SEARCH WITH K-FOLD CROSS VALIDATION TO CHOOSE THE BEST HYPERPARAMS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYA7iHsIpPLQ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Oh0ms-pPN-"
      },
      "source": [
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxS3XGM1pPQo",
        "outputId": "9fd623e3-de00-4fab-87c8-0e37cb020682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Models\n",
        "lr_params = {\"penalty\": [\"l1\", \"l2\"]}\n",
        "lr = LogisticRegression()\n",
        "\n",
        "rfc_params = {\"n_estimators\": [3, 5, 10, 15],\n",
        "              \"max_depth\": [2, 3, 4, 5],\n",
        "              \"min_samples_split\": [3, 5, 7, 9]}\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "gbc_params = {\"n_estimators\": [3, 5, 10, 15],\n",
        "              \"max_depth\": [2, 3, 4, 5],\n",
        "              \"min_samples_split\": [3, 5, 7, 9]}\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "\n",
        "clf_rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
        "clf_rfc.fit(X_train, y_train)\n",
        "\n",
        "clf_gbc = GridSearchCV(gbc, gbc_params, cv=5)\n",
        "clf_gbc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"\\n----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', clf_lr.score(X_train, y_train))\n",
        "print('Test set score:', clf_lr.score(X_test, y_test))\n",
        "\n",
        "print(\"\\n----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', clf_rfc.score(X_train, y_train))\n",
        "print('Test set score:', clf_rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"\\n----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', clf_gbc.score(X_train, y_train))\n",
        "print('Test set score:', clf_gbc.score(X_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.9261687571265679\n",
            "Test set score: 0.8641025641025641\n",
            "\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.6889965792474344\n",
            "Test set score: 0.6901709401709402\n",
            "\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.7887685290763968\n",
            "Test set score: 0.7893162393162393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGjMeYJ2pPTm"
      },
      "source": [
        "# USING 1-GRAM AND 2 - GRAM features together as a feature set. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQiMrcYhpPWY",
        "outputId": "cd19d0f5-e99c-4138-ec74-de7b64a9f9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(sentences[\"text\"])\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "sentences = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
        "sentences.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1st</th>\n",
              "      <th>29th</th>\n",
              "      <th>29th september</th>\n",
              "      <th>abbreviation</th>\n",
              "      <th>abbreviation living</th>\n",
              "      <th>abdication</th>\n",
              "      <th>abdication neighbour</th>\n",
              "      <th>abide</th>\n",
              "      <th>abide consequence</th>\n",
              "      <th>abide figure</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability affection</th>\n",
              "      <th>ability awkwardness</th>\n",
              "      <th>ability difficulty</th>\n",
              "      <th>able</th>\n",
              "      <th>able attempt</th>\n",
              "      <th>able avail</th>\n",
              "      <th>able avoid</th>\n",
              "      <th>able bear</th>\n",
              "      <th>able convince</th>\n",
              "      <th>able devise</th>\n",
              "      <th>able eat</th>\n",
              "      <th>able far</th>\n",
              "      <th>able feign</th>\n",
              "      <th>able join</th>\n",
              "      <th>able judge</th>\n",
              "      <th>able leave</th>\n",
              "      <th>able letter</th>\n",
              "      <th>able live</th>\n",
              "      <th>able marry</th>\n",
              "      <th>able persuade</th>\n",
              "      <th>able regard</th>\n",
              "      <th>able remain</th>\n",
              "      <th>able return</th>\n",
              "      <th>able ring</th>\n",
              "      <th>able rise</th>\n",
              "      <th>able set</th>\n",
              "      <th>able shew</th>\n",
              "      <th>able speak</th>\n",
              "      <th>able tell</th>\n",
              "      <th>...</th>\n",
              "      <th>young young</th>\n",
              "      <th>younker</th>\n",
              "      <th>youth</th>\n",
              "      <th>youth beauty</th>\n",
              "      <th>youth bloom</th>\n",
              "      <th>youth early</th>\n",
              "      <th>youth father</th>\n",
              "      <th>youth fine</th>\n",
              "      <th>youth hardly</th>\n",
              "      <th>youth hope</th>\n",
              "      <th>youth jaw</th>\n",
              "      <th>youth kill</th>\n",
              "      <th>youth learn</th>\n",
              "      <th>youth like</th>\n",
              "      <th>youth mention</th>\n",
              "      <th>youth possibly</th>\n",
              "      <th>youth restore</th>\n",
              "      <th>youth say</th>\n",
              "      <th>youth spring</th>\n",
              "      <th>youth value</th>\n",
              "      <th>youth vigour</th>\n",
              "      <th>youthful</th>\n",
              "      <th>youthful infatuation</th>\n",
              "      <th>zeal</th>\n",
              "      <th>zeal business</th>\n",
              "      <th>zeal common</th>\n",
              "      <th>zeal dwell</th>\n",
              "      <th>zeal sport</th>\n",
              "      <th>zeal think</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zealand australia</th>\n",
              "      <th>zealous</th>\n",
              "      <th>zealous officer</th>\n",
              "      <th>zealous subject</th>\n",
              "      <th>zealously</th>\n",
              "      <th>zealously discharge</th>\n",
              "      <th>zigzag</th>\n",
              "      <th>zigzag go</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>remarkable Alice think way hear Rabbit oh dear</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>oh dear</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>shall late</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 35313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   1st  29th  ...                                               text   author\n",
              "0    0     0  ...  Alice begin tired sit sister bank have twice p...  Carroll\n",
              "1    0     0  ...  consider mind hot day feel sleepy stupid pleas...  Carroll\n",
              "2    0     0  ...     remarkable Alice think way hear Rabbit oh dear  Carroll\n",
              "3    0     0  ...                                            oh dear  Carroll\n",
              "4    0     0  ...                                         shall late  Carroll\n",
              "\n",
              "[5 rows x 35313 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrRHFK12pPZH",
        "outputId": "c8751692-716c-4b91-c41a-0519e9311306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=55)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.9438064989005619\n",
            "\n",
            "Test set score: 0.8592592592592593\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9692157341803078\n",
            "\n",
            "Test set score: 0.8472934472934472\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8248228683117518\n",
            "\n",
            "Test set score: 0.807977207977208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-_Ukcc5pPbt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwy-IDBQpPey"
      },
      "source": [
        "#better model with 1 and 2 gram features"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMrTEvRqpPh7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}