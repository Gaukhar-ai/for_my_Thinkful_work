{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Word2Vec.models.gensimModels.Alice.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaukhar-ai/for_my_Thinkful_work/blob/master/Word2Vec_models_gensimModels_Alice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NMp7DkyT5_5"
      },
      "source": [
        "# Solutions to Checkpoint 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9znCVHJRT5_6"
      },
      "source": [
        "## 1. Train your own word2vec representations as we did in our first example in the checkpoint. But, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BjP-u3cT5_7",
        "outputId": "d5dab966-9fe1-46bd-d0bb-d985c8160786"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import gensim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/mladmin/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages (2.1.0)\n",
            "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/en_core_web_sm\n",
            "-->\n",
            "/Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WaqFGWPT5__"
      },
      "source": [
        "# utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    # visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzmxBCnkT6AB"
      },
      "source": [
        "# load and clean the data\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# the chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDzp6gNKT6AE"
      },
      "source": [
        "# parse the cleaned novels. This can take a bit.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWKcDf0zT6AG",
        "outputId": "d8791d6c-f8a6-4bc0-d506-feba7cba7252"
      },
      "source": [
        "# group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# combine the sentences from the two novels into one data frame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "sentences.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                                      (Oh, dear, !)  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBIQ106wT6AJ"
      },
      "source": [
        "# get rid off stop words and punctuation\n",
        "# and lemmatize the tokens\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W87FS__wT6AL"
      },
      "source": [
        "Below, we train several word2vec models. In particular, models 1 through 3 try windows sizes of 4, 6 and 8 and models 4 through 6 try vector size of 200 instead of 100:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-JsyaRvT6AL"
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model1 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=4,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model2 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model3 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=8,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model4 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=4,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model5 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model6 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=8,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLCkmv4gT6AO"
      },
      "source": [
        "word2vec_arr1 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr2 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr3 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr4 = np.zeros((sentences.shape[0],200))\n",
        "word2vec_arr5 = np.zeros((sentences.shape[0],200))\n",
        "word2vec_arr6 = np.zeros((sentences.shape[0],200))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr1[i,:] = np.mean([model1[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr2[i,:] = np.mean([model2[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr3[i,:] = np.mean([model3[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr4[i,:] = np.mean([model4[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr5[i,:] = np.mean([model5[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr6[i,:] = np.mean([model6[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr1 = pd.DataFrame(word2vec_arr1)\n",
        "word2vec_arr2 = pd.DataFrame(word2vec_arr2)\n",
        "word2vec_arr3 = pd.DataFrame(word2vec_arr3)\n",
        "word2vec_arr4 = pd.DataFrame(word2vec_arr4)\n",
        "word2vec_arr5 = pd.DataFrame(word2vec_arr5)\n",
        "word2vec_arr6 = pd.DataFrame(word2vec_arr6)\n",
        "\n",
        "sentences1 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr1], axis=1)\n",
        "sentences1.dropna(inplace=True)\n",
        "\n",
        "sentences2 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr2], axis=1)\n",
        "sentences2.dropna(inplace=True)\n",
        "\n",
        "sentences3 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr3], axis=1)\n",
        "sentences3.dropna(inplace=True)\n",
        "\n",
        "sentences4 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr4], axis=1)\n",
        "sentences4.dropna(inplace=True)\n",
        "\n",
        "sentences5 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr5], axis=1)\n",
        "sentences5.dropna(inplace=True)\n",
        "\n",
        "sentences6 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr6], axis=1)\n",
        "sentences6.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Xk2f1qT6AQ",
        "outputId": "76bde5bd-f9a1-40b1-c419-f4b96eeee1a5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y1 = sentences1['author']\n",
        "Y2 = sentences2['author']\n",
        "Y3 = sentences3['author']\n",
        "Y4 = sentences4['author']\n",
        "Y5 = sentences5['author']\n",
        "Y6 = sentences6['author']\n",
        "\n",
        "X1 = np.array(sentences1.drop(['text','author'], 1))\n",
        "X2 = np.array(sentences2.drop(['text','author'], 1))\n",
        "X3 = np.array(sentences3.drop(['text','author'], 1))\n",
        "X4 = np.array(sentences4.drop(['text','author'], 1))\n",
        "X5 = np.array(sentences5.drop(['text','author'], 1))\n",
        "X6 = np.array(sentences6.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.4, random_state=123)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.4, random_state=123)\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, Y3, test_size=0.4, random_state=123)\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, Y4, test_size=0.4, random_state=123)\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X5, Y5, test_size=0.4, random_state=123)\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X6, Y6, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "print(\"-----------------------Word2vec Model 1------------------------------\")\n",
        "lr.fit(X_train1, y_train1)\n",
        "rfc.fit(X_train1, y_train1)\n",
        "gbc.fit(X_train1, y_train1)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', lr.score(X_test1, y_test1))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', rfc.score(X_test1, y_test1))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', gbc.score(X_test1, y_test1))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 2------------------------------\")\n",
        "lr.fit(X_train2, y_train2)\n",
        "rfc.fit(X_train2, y_train2)\n",
        "gbc.fit(X_train2, y_train2)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', lr.score(X_test2, y_test2))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', rfc.score(X_test2, y_test2))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', gbc.score(X_test2, y_test2))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 3------------------------------\")\n",
        "lr.fit(X_train3, y_train3)\n",
        "rfc.fit(X_train3, y_train3)\n",
        "gbc.fit(X_train3, y_train3)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', lr.score(X_test3, y_test3))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', rfc.score(X_test3, y_test3))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', gbc.score(X_test3, y_test3))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 4------------------------------\")\n",
        "lr.fit(X_train4, y_train4)\n",
        "rfc.fit(X_train4, y_train4)\n",
        "gbc.fit(X_train4, y_train4)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', lr.score(X_test4, y_test4))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', rfc.score(X_test4, y_test4))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', gbc.score(X_test4, y_test4))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 5------------------------------\")\n",
        "lr.fit(X_train5, y_train5)\n",
        "rfc.fit(X_train5, y_train5)\n",
        "gbc.fit(X_train5, y_train5)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', lr.score(X_test5, y_test5))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', rfc.score(X_test5, y_test5))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', gbc.score(X_test5, y_test5))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 6------------------------------\")\n",
        "lr.fit(X_train6, y_train6)\n",
        "rfc.fit(X_train6, y_train6)\n",
        "gbc.fit(X_train6, y_train6)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', lr.score(X_test6, y_test6))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', rfc.score(X_test6, y_test6))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', gbc.score(X_test6, y_test6))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------Word2vec Model 1------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7900605288308379\n",
            "\n",
            "Test set score: 0.7793696275071633\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9850270786874801\n",
            "\n",
            "Test set score: 0.8127984718242598\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9015610066900287\n",
            "\n",
            "Test set score: 0.8228271251193887\n",
            "-----------------------Word2vec Model 2------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.8069448869066582\n",
            "\n",
            "Test set score: 0.7975167144221585\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9840713603058299\n",
            "\n",
            "Test set score: 0.8118433619866284\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9018795794839121\n",
            "\n",
            "Test set score: 0.8228271251193887\n",
            "-----------------------Word2vec Model 3------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.810767760433259\n",
            "\n",
            "Test set score: 0.8065902578796562\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9812042051608793\n",
            "\n",
            "Test set score: 0.8137535816618912\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9079324625676968\n",
            "\n",
            "Test set score: 0.8347659980897804\n",
            "-----------------------Word2vec Model 4------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7897419560369544\n",
            "\n",
            "Test set score: 0.7750716332378224\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9812042051608793\n",
            "\n",
            "Test set score: 0.8127984718242598\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9123924816820643\n",
            "\n",
            "Test set score: 0.833810888252149\n",
            "-----------------------Word2vec Model 5------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7938834023574387\n",
            "\n",
            "Test set score: 0.788443170964661\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9812042051608793\n",
            "\n",
            "Test set score: 0.8233046800382043\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.908888180949347\n",
            "\n",
            "Test set score: 0.8385864374403056\n",
            "-----------------------Word2vec Model 6------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7992991398534565\n",
            "\n",
            "Test set score: 0.791308500477555\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9780184772220453\n",
            "\n",
            "Test set score: 0.8276026743075454\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.921631092704683\n",
            "\n",
            "Test set score: 0.8404966571155683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRskAdST6AS"
      },
      "source": [
        "Model 6's performance seems to be better. In particular, the best test performance is achieved using model 6 and gradient boosting. Three random forest models also achieved the highest score when trained on model 6. \n",
        "\n",
        "Moreover, model 6's performance is also superior to that of the model in the checkpoint."
      ]
    }
  ]
}