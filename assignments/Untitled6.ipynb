{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n",
    "                                             PROBABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def function_name(inputs):\n",
    "    process\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inputs\n",
    "None, we're just flipping a coin\n",
    "Outputs\n",
    "'H' or 'T' (50% chance of each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Google for how we learned about random.choice:\n",
    "\"python list select random element\"\n",
    "sample_space = [\"H\", \"T\"]\n",
    "random.choice(sample_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def coin_flip():\n",
    "    sample_space = [\"H\", \"T\"]\n",
    "    outcome = random.choice(sample_space)\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_trials = 500\n",
    "heads = 0\n",
    "tails = 0\n",
    "\n",
    "for i in range(n_trials):\n",
    "    outcome = coin_flip()\n",
    "    if outcome == \"T\":\n",
    "        tails += 1\n",
    "    elif outcome == \"H\":\n",
    "        heads += 1\n",
    "\n",
    "print(\"P(head)\")\n",
    "heads / n_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability the 'species' is virginica?\n",
    "iris[\"species\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability the 'petal_length' is greater than 2.0?\n",
    "greater_than = iris[\"petal_length\"] > 2.0\n",
    "greater_than.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " We can do math with True/False\n",
    "#Trues will act as 1s in this math\n",
    "#Falses will act as 0s in this math\n",
    "\n",
    " sum([True, True, False]) is 2\n",
    " the average of [True, True, False] is 0.66\n",
    "#(average can be used in our case)\n",
    "greater_than.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_trials = 1000\n",
    "\n",
    "change_wins = 0\n",
    "stay_wins = 0\n",
    "\n",
    "#'Play' the game repeatedly\n",
    "for i in range(n_trials):\n",
    "    options = [\"goat\", \"goat\", \"car\"]\n",
    "    \n",
    "    # Randomize which door each item will be behind\n",
    "    random.shuffle(options)\n",
    "\n",
    "    # We will always choose the first door as our choice\n",
    "    chosen = options[0]\n",
    "    \n",
    "    # Identify the remaining 2 options using 'slicing'\n",
    "    remaining = options[1:]\n",
    "    \n",
    "    # Find out where the goat is located in the remaining options\n",
    "    goat_index = remaining.index(\"goat\")\n",
    "    # Remove the/a goat from the remaining options\n",
    "    del remaining[goat_index]\n",
    "    \n",
    "    # Add 1 to the strategy that would have won this round of the game\n",
    "    if chosen == \"car\":\n",
    "        stay_wins += 1\n",
    "    else:\n",
    "        change_wins += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_trials = 1000\n",
    "\n",
    "change_wins = 0\n",
    "stay_wins = 0\n",
    "\n",
    "\n",
    "for i in range(n_trials):\n",
    "    options = [\"goat\", \"goat\", \"car\"]\n",
    "    random.shuffle(options)\n",
    "\n",
    "    # Being lazy and just taking 1st door always\n",
    "    # Should not negatively impact the simulation\n",
    "    chosen = options[0]\n",
    "    remaining = options[1:]\n",
    "\n",
    "    goat_index = remaining.index(\"goat\")\n",
    "    del remaining[goat_index]\n",
    "\n",
    "    if chosen == \"car\":\n",
    "        stay_wins += 1\n",
    "    else:\n",
    "        change_wins += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Prob of winning if stay: {stay_wins / n_trials}\")\n",
    "print(f\"Prob of winning if change: {change_wins / n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "adjective0 = \"cool\"\n",
    "adjective1 = \"gorgeous\"\n",
    "noun = \"python\"\n",
    "\n",
    "print(f\"I like {noun} because it is very {adjective0} & {adjective1}\")\n",
    "print(\"I like {} because it is very {} & {}\".format(noun, adjective0, adjective1))\n",
    "print(\"I like \" + noun + \" because it is very \" + adjective0 + \" & \" + adjective1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_notebooks/week1/1_3_B_        DESCRIPTIVE_STATS.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/nba_player_seasons.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba = pd.read_csv(data_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(n_rows, n_cols)\n",
    " (height, width)\n",
    "nba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "avg = nba[\"PTS\"].mean()\n",
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "med = nba[\"PTS\"].median()\n",
    "med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode = nba[\"PTS\"].astype(int).mode()\n",
    "\n",
    ".mode() returns a series\n",
    " this is to pull out first item of series for easy plotting\n",
    "mode = mode[0]\n",
    "mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: use matplotlib\n",
    "plt.hist(nba[\"PTS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: use pandas\n",
    "nba[\"PTS\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Option 3: use seaborn\n",
    "sns.distplot(nba[\"PTS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making it fancy by including our numeric measures\n",
    "plt.hist(nba[\"PTS\"], bins=10)\n",
    "plt.axvline(avg, c=\"red\", label=\"mean\")\n",
    "plt.axvline(med, c=\"green\", label=\"median\")\n",
    "plt.axvline(mode, c=\"orange\", label=\"mode (as int)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot(nba[\"ORB\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percents = nba.filter(like=\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percents.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percents = percents.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percents.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    IQR - inter quartile range\n",
    "    Outliers - 1.5 * IQR away from median\n",
    "percents.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Standard deviation is the\n",
    "    average distance each observation is from the mean\n",
    "\n",
    "    if our data is\n",
    "x = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     this makes our average\n",
    "avg = np.mean(x)\n",
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    the distances from the mean are\n",
    "dists = avg - x\n",
    "dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    we dont want negatives here so we'll square these\n",
    "sq_dists = dists ** 2\n",
    "sq_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we'll take the average of these distances\n",
    "avg_sq_dist = np.mean(sq_dists)\n",
    "avg_sq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    we squared these numbers so now we want to undo that squaring\n",
    "    and this is our standard deviation\n",
    "np.sqrt(avg_sq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confirm that this is the same number\n",
    "np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.describe?\n",
    "#help(nba.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.describe(include=\"O\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba[[\"PTS\", \"Player\"]].groupby(\"Player\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggs = {\"PTS\": [\"mean\", \"median\", \"count\"]}\n",
    "nba.groupby(\"Player\").agg(aggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nba.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Note, not all of these values are on the same scale\n",
    "    (i.e. the max FG% is 1, but PTS can have higher values)\n",
    "     This being on different scales would be an issue.\n",
    "     How might you put all of these columns on the same scale\n",
    "    #before creating your rating?\n",
    "nba[\"bigman_rating\"] = (\n",
    "    nba[\"PTS\"] * 0.3\n",
    "    + nba[\"ORB\"] * 0.15\n",
    "    + nba[\"DRB\"] * 0.15\n",
    "    + nba[\"FG%\"] * 0.1\n",
    "    + nba[\"BLK\"] * 0.3\n",
    ")\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                   Inferential week1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/1-UsRiETIQzkT5LWICVx3t-w9_-EgW5giL3K6lVkimYQ/export?format=csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "titanic = pd.read_csv(data_url)\n",
    "titanic.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic[\"Age\"].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic[\"Survived\"].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "titanic[\"surv_str\"] = titanic[\"Survived\"].replace({0: \"No\", 1: \"Yes\"})\n",
    "titanic[\"surv_str\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic[\"Survived\"].corr(titanic[\"Pclass\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crosstab = pd.crosstab(titanic[\"Survived\"], titanic[\"Pclass\"])\n",
    "sns.heatmap(crosstab, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sns.distplot(titanic[\"Fare\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_size = 50\n",
    "\n",
    "sample = titanic.sample(sample_size)\n",
    "sample[\"Fare\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_size = 500\n",
    "n_samples = 100\n",
    "\n",
    "sample_means = []\n",
    "for i in range(n_samples):\n",
    "    sample = titanic.sample(sample_size)\n",
    "    sample_mean = sample[\"Fare\"].mean()\n",
    "\n",
    "    sample_means.append(sample_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot(sample_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titanic[\"Survived\"].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample = np.random.beta(1, 1, 10000)\n",
    "sns.distplot(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs_n_survived = titanic[\"Survived\"].sum()\n",
    "n_passengers = titanic[\"PassengerId\"].nunique()\n",
    "obs_rate = titanic[\"Survived\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_rates = []\n",
    "\n",
    "for _ in range(100000):\n",
    "    # Prior\n",
    "    rate = np.random.beta(1, 1)\n",
    "\n",
    "    # Generative process\n",
    "    gen_n_survived = np.random.binomial(n_passengers, rate)\n",
    "\n",
    "    if gen_n_survived == obs_n_survived:\n",
    "        valid_rates.append(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sns.distplot(valid_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    np.random.beta?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 15\n",
    "beta = 13\n",
    "\n",
    "sns.distplot(np.random.beta(alpha, beta, 100000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "valid_rates = []\n",
    "\n",
    "for _ in range(1000000):\n",
    "    # Prior\n",
    "    rate = np.random.beta(alpha, beta)\n",
    "\n",
    "    # Generative process\n",
    "    gen_n_survived = np.random.binomial(n_passengers, rate)\n",
    "\n",
    "    if gen_n_survived == obs_n_survived:\n",
    "        valid_rates.append(rate)\n",
    "\n",
    "    Plot the posterior\n",
    "sns.distplot(valid_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "valid_rates = []\n",
    "\n",
    "for _ in tqdm(range(1000000)):\n",
    "    # Prior\n",
    "    rate = np.random.beta(alpha, beta)\n",
    "\n",
    "    # Generative process\n",
    "    gen_n_survived = np.random.binomial(n_passengers, rate)\n",
    "\n",
    "    if gen_n_survived == obs_n_survived:\n",
    "        valid_rates.append(rate)\n",
    "\n",
    "    Plot the posterior\n",
    "sns.distplot(valid_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                      DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "    Reproduce same exact results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    np.random.poisson?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect for 10,000 people to arrive every hour\n",
    "n = 10000\n",
    "time = 1  # in hours\n",
    "\n",
    "\n",
    "lmbda = n / time\n",
    "lmbda = lmbda / 60 / 60  # in seconds\n",
    "\n",
    "print(f\"Expect to see {lmbda} people per second ({n} per hour)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Simulate an hours worth of arrivals\n",
    "arrivals = np.random.poisson(lmbda, size=3600)\n",
    "\n",
    "print(\"From our simulation:\")\n",
    "print(f\"* {arrivals.sum()} people arrived in an hour\")\n",
    "print(f\"* {arrivals.mean()} was the average number arriving per second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Intentionally high bin count to highlight\n",
    "    that the distribution is discrete\n",
    "bins = 2 * len(set(arrivals)) - 1\n",
    "\n",
    "plt.hist(arrivals, bins=bins)\n",
    "plt.axvline(arrivals.mean(), c=\"orange\")\n",
    "plt.title(\"Arrivals per second\")\n",
    "plt.show()\n",
    "\n",
    "total_people_arrived = np.cumsum(arrivals[1:30])\n",
    "plt.step(range(len(total_people_arrived)), total_people_arrived)\n",
    "plt.title(\"Total people in stadium over time (first 30 seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # np.random.binomial?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # How many times will 'rock' appear per `n` games of RPS\n",
    "n = 100\n",
    "p = 1 / 3  # Assuming rock is random it will occur a third of the time\n",
    "\n",
    "print(f\"Expect to see {n * p:.2f} rocks per {n} rounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Simulate' 100 RPS games 1000 times\n",
    "rocks_thrown = np.random.binomial(n, p, size=1000)\n",
    "print(\"From our simulation:\")\n",
    "print(\n",
    "    f\"* {rocks_thrown.mean():.2f} was the average number of rocks thrown per {n} games.\"\n",
    ")\n",
    "\n",
    "    # Intentionally high bin count to highlight\n",
    "    # that the distribution is discrete\n",
    "bins = 2 * len(set(rocks_thrown))\n",
    "\n",
    "plt.hist(rocks_thrown, bins=bins)\n",
    "plt.axvline(rocks_thrown.mean(), c=\"orange\")\n",
    "plt.title(f\"Rocks thrown per {n} rounds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "n_stops = 10000\n",
    "n_rounds = 5\n",
    "n_hands = 10\n",
    "p_of_rock = 1 / 3\n",
    "\n",
    "print(f\"Expect to see {p_of_rock * n_hands} rocks per round.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rocks_per_round = []\n",
    "avg_rocks_per_round = []\n",
    "for _ in range(n_stops):\n",
    "    rocks_per_round_this_stop = np.random.binomial(n_hands, p_of_rock, size=n_rounds)\n",
    "\n",
    "    rocks_per_round.extend(rocks_per_round_this_stop)\n",
    "    avg_rocks_per_round.append(rocks_per_round_this_stop.mean())\n",
    "\n",
    "print(\"From our simulation:\")\n",
    "print(f\"* {np.mean(rocks_per_round)} was the average number of rocks thrown per round.\")\n",
    "\n",
    "sns.distplot(avg_rocks_per_round, fit=stats.norm)\n",
    "plt.axvline(np.mean(avg_rocks_per_round), c=\"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = 10000\n",
    "x = np.random.beta(0.5, 0.5, n)\n",
    "\n",
    "plt.hist(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_samples = 1000\n",
    "sample_size = 10000\n",
    "replacement = True\n",
    "\n",
    "means = []\n",
    "for i in range(n_samples):\n",
    "    sample = np.random.choice(x, sample_size, replacement)\n",
    "    mean = sample.mean()\n",
    "    means.append(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true_mean = x.mean()\n",
    "mean_est = np.mean(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(means)\n",
    "plt.axvline(true_mean, c=\"orange\")\n",
    "plt.axvline(mean_est, c=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "np.random.seed(1337)\n",
    "x = np.random.normal(size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_size = 100\n",
    "sample = np.random.choice(x, sample_size)\n",
    "\n",
    "print(f\"Population mean: {np.mean(x)}\")\n",
    "print(f\"Sample mean: {np.mean(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ci_hi = sample.mean() + 1.96 * sample.std() / np.sqrt(sample_size)\n",
    "ci_lo = sample.mean() - 1.96 * sample.std() / np.sqrt(sample_size)\n",
    "\n",
    "true_mean = np.mean(x)\n",
    "captured_true_mean = true_mean > ci_lo and true_mean < ci_hi\n",
    "\n",
    "print(f\"({ci_lo}, {ci_hi})\\n\\nCaptures true mean: {captured_true_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.t.interval(0.95, len(sample) - 1, loc=np.mean(sample), scale=stats.sem(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def ci_mean(sample, confidence=0.95):\n",
    "    ci = stats.t.interval(\n",
    "        confidence, len(sample) - 1, loc=np.mean(sample), scale=stats.sem(sample)\n",
    "    )\n",
    "\n",
    "    return ci\n",
    "\n",
    "\n",
    "ci_mean(sample, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confidence_levels = [0.10, 0.90, 0.95, 0.99, 0.99999999]\n",
    "for c in confidence_levels:\n",
    "    ci = ci_mean(sample, c)\n",
    "    print(f\"{c * 100}% CI: {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(1337)\n",
    "x = np.random.normal(size=10000)\n",
    "pop_mean = x.mean()\n",
    "\n",
    "sample_size = 5\n",
    "\n",
    "captured = []\n",
    "\n",
    "for _ in range(100):\n",
    "    sample = np.random.choice(x, sample_size)\n",
    "\n",
    "    ci_lo, ci_hi = ci_mean(sample, 0.95)\n",
    "\n",
    "    captured_true_mean = pop_mean > ci_lo and pop_mean < ci_hi\n",
    "    captured.append(captured_true_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum(captured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         fortune_100_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/1w1BfAGjEJk_Ywglo0nWKUk5gKmqEmfFb101IoXtj8D8/export?format=csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(data_url)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Dashes are used as missing value indicator in some columns\n",
    "    # replacing these dashes with NaN\n",
    "df[df == \"-\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "num_cols = [\n",
    "    \"Revenues ($M)\",\n",
    "    \"Profits ($M)\",\n",
    "    \"Assets ($M)\",\n",
    "    \"Mkt Value as of 3/29/18 ($M)\",\n",
    "    \"Employees\",\n",
    "]\n",
    "\n",
    "for num_col in num_cols:\n",
    "    df[num_col] = df[num_col].str.replace(\"$\", \"\", regex=False)\n",
    "    df[num_col] = df[num_col].str.replace(\",\", \"\", regex=False)\n",
    "    df[num_col] = pd.to_numeric(df[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "percent_cols = [\"Profit Change\", \"Revenue Change\"]\n",
    "\n",
    "for percent_col in percent_cols:\n",
    "    df[percent_col] = df[percent_col].str.replace(\"%\", \"\", regex=False)\n",
    "    df[percent_col] = pd.to_numeric(df[percent_col])\n",
    "    df[percent_col] /= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df.apply(pd.to_numeric, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    find biggest winners/losers\n",
    "df[\"rank_diff\"] = df[\"Previous Rank\"] - df[\"rank\"]\n",
    "df.sort_values(\"rank_diff\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = 3\n",
    "sector_counts = df[\"Sector\"].value_counts()\n",
    "top_n_sectors = sector_counts.head(3).index\n",
    "\n",
    "plot_cols = [\"rank\", \"Revenues ($M)\", \"Employees\", \"Profits ($M)\", \"Sector\"]\n",
    "\n",
    "plot_df = df.loc[df[\"Sector\"].isin(top_n_sectors), plot_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.pairplot(plot_df, hue=\"Sector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def tall_corr(df):\n",
    "    \"\"\"View corrs as a tall dataframe sorted by absolute value\"\"\"\n",
    "    # Run correlation\n",
    "    corr_df = df.corr()\n",
    "\n",
    "    # Set upper triangle to np.nan\n",
    "    is_upper_tri = np.triu(np.ones(corr_df.shape)).astype(bool)\n",
    "    corr_df[is_upper_tri] = np.nan\n",
    "\n",
    "    # Convert to tall\n",
    "    corr_df = corr_df.reset_index()\n",
    "    tall_corr_df = pd.melt(corr_df, id_vars=\"index\")\n",
    "    tall_corr_df.columns = [\"a\", \"b\", \"corr\"]\n",
    "\n",
    "    # Sort by absolute value of corr\n",
    "    tall_corr_df[\"abs_corr\"] = tall_corr_df[\"corr\"].abs()\n",
    "    tall_corr_df = tall_corr_df.sort_values(\"abs_corr\", ascending=False)\n",
    "    tall_corr_df = tall_corr_df.drop(columns=\"abs_corr\")\n",
    "\n",
    "    return tall_corr_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corr_df = tall_corr(df)\n",
    "\n",
    "    # View top 10 colored by\n",
    "corr_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                          Python Review Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def dict_sum(d, thresh):\n",
    "    for i in d:\n",
    "        print('{} {} {}'.format(i, d[i], thresh))\n",
    "        sum = 0\n",
    "        if (i >= thresh):\n",
    "            sum += d[i]\n",
    "            print(sum)\n",
    "    return sum\n",
    "\n",
    "dict_sum({1: 2, 3: 4, 5: 6}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mtcars_url = 'https://gist.githubusercontent.com/ZeccaLehn/4e06d2575eb9589dbe8c365d61cb056c/raw/64f1660f38ef523b2a1a13be77b002b98665cdfe/mtcars.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Using table: fueleconomy.public.vehicles\n",
    "-- No context for these prompts :(  Straight to business.\n",
    "\n",
    "-- How many records are in the vehicles table?\n",
    "SELECT COUNT(*)\n",
    "FROM vehicles;\n",
    "\n",
    "-- How many records are there that use Diesel fuel?\n",
    "SELECT COUNT(*)\n",
    "FROM vehicles\n",
    "WHERE fuel = 'Diesel';\n",
    "\n",
    "\n",
    "-- How many non-NULL records are there in the cyl column?\n",
    "-- 33442\n",
    "SELECT COUNT(*)\n",
    "FROM vehicles;\n",
    "\n",
    "-- 33384\n",
    "SELECT COUNT(cyl)\n",
    "FROM vehicles;\n",
    "\n",
    "-- 33384\n",
    "SELECT COUNT(*)\n",
    "FROM vehicles\n",
    "WHERE cyl IS NOT NULL;\n",
    "\n",
    "\n",
    "-- Can you explain why the cyl column has NULLs?\n",
    "SELECT *\n",
    "FROM vehicles\n",
    "WHERE cyl IS NULL;\n",
    "\n",
    "\n",
    "-- List the unique fuel types when cyl is NULL\n",
    "SELECT DISTINCT fuel\n",
    "FROM vehicles\n",
    "WHERE cyl IS NULL;\n",
    "\n",
    "\n",
    "-- Count the unique number of makes when cyl is NULL\n",
    "SELECT COUNT(DISTINCT make), COUNT(model), COUNT(DISTINCT model)\n",
    "FROM vehicles\n",
    "WHERE cyl IS NULL;\n",
    "\n",
    "\n",
    "-- Find unique make model pairs where cyl is NULL\n",
    "SELECT DISTINCT make, model\n",
    "FROM vehicles\n",
    "WHERE cyl IS NULL;\n",
    "\n",
    "\n",
    "-- What is the average hwy mpg? cty mpg?\n",
    "SELECT ROUND(AVG(hwy), 2), ROUND(AVG(cty), 2)\n",
    "FROM vehicles;\n",
    "\n",
    "\n",
    "-- What brands make the most fuel efficient cars?\n",
    "-- Include a count of the number of records for each make to get\n",
    "-- additional perspective?\n",
    "SELECT make,\n",
    "       ROUND(AVG(hwy), 2) AS avg_hwy, \n",
    "       ROUND(AVG(cty), 2) AS avg_cty,\n",
    "\t   COUNT(*)\n",
    "FROM vehicles\n",
    "GROUP BY make\n",
    "ORDER BY avg_hwy DESC, avg_cty DESC;\n",
    "\n",
    "\n",
    "-- Redo the last analysis but with a minimum number of records for\n",
    "-- a make to be elligible.  Let's say 1000 to be considered.\n",
    "SELECT make,\n",
    "       ROUND(AVG(hwy), 2) AS avg_hwy, \n",
    "       ROUND(AVG(cty), 2) AS avg_cty,\n",
    "\t   COUNT(*)\n",
    "FROM vehicles\n",
    "GROUP BY make\n",
    "HAVING COUNT(*) > 1000\n",
    "ORDER BY avg_hwy DESC, avg_cty DESC;\n",
    "\n",
    "\n",
    "-- Redo the last analysis but only consider any type of 'Regular' fuel.\n",
    "-- So our question were answering now.  Which large* makes produce\n",
    "-- the most fuel efficient cars (where large is >1000 records)?\n",
    "SELECT make,\n",
    "       ROUND(AVG(hwy), 2) AS avg_hwy, \n",
    "       ROUND(AVG(cty), 2) AS avg_cty,\n",
    "\t   COUNT(*)\n",
    "FROM vehicles\n",
    "WHERE fuel ILIKE '%regular%'\n",
    "GROUP BY make\n",
    "HAVING COUNT(*) > 1000\n",
    "ORDER BY avg_hwy DESC, avg_cty DESC;\n",
    "\n",
    "-- What fuel types did we even exclude above?\n",
    "SELECT DISTINCT fuel\n",
    "FROM vehicles\n",
    "WHERE NOT fuel ILIKE '%regular%';\n",
    "\n",
    "\n",
    "-- What are the top 5 most fuel efficient cars that\n",
    "-- have any type of four-wheel or all-wheel drive?\n",
    "\n",
    "-- Note, multiple records for make/model/year combination\n",
    "-- use business sense and client as a resource of best agg function\n",
    "SELECT\n",
    "\tmake,\n",
    "\tmodel,\n",
    "\tyear,\n",
    "\tROUND(AVG(hwy), 2) AS avg_hwy_mpg,\n",
    "\tROUND(AVG(cty), 2) AS avg_cty_mpg,\n",
    "\tCOUNT(*) AS n\n",
    "FROM \n",
    "\tvehicles\n",
    "WHERE drive LIKE '%All%' OR drive LIKE '%4%'\n",
    "GROUP BY \n",
    "\tmodel, make, year\n",
    "ORDER BY n DESC, avg_hwy_mpg DESC, avg_cty_mpg DESC\n",
    "LIMIT 5;\n",
    "\n",
    "\n",
    "-- Create a report that shows the worst & best hwy mpg before 1990\n",
    "-- and after 1990\n",
    "SELECT CASE\n",
    "         WHEN year > 1990 THEN 'After 1990'\n",
    "         WHEN year < 1990 THEN 'Before 1990'\n",
    "\t\t ELSE '1990'\n",
    "       END AS year_label,\n",
    "\t   MIN(hwy) AS worst_hwy_mpg,\n",
    "\t   MAX(hwy) AS best_hwy_mpg,\n",
    "\t   COUNT(*) AS n\n",
    "FROM vehicles\n",
    "WHERE NOT fuel ILIKE '%Electricity%'\n",
    "GROUP BY year_label;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "-- Using the band_members database\n",
    "\n",
    "----------------------------------\n",
    "-- JOINs -------------------------\n",
    "----------------------------------\n",
    "\n",
    "-- Explore the 2 tables in the band_members database.\n",
    "-- How can we combine the information from these tables?\n",
    "SELECT *\n",
    "FROM band_instruments;\n",
    "\n",
    "SELECT *\n",
    "FROM band_members;\n",
    "\n",
    "-- Join the 2 tables and only keep records\n",
    "-- where the key is found in both tables\n",
    "SELECT *\n",
    "FROM band_members \n",
    "JOIN band_instruments\n",
    "ON band_members.name = band_instruments.name;\n",
    "\n",
    "-- SELECT * brings back our key column twice!\n",
    "-- Be explicit to avoid confusing field naming.\n",
    "SELECT band_members.name, band, plays\n",
    "FROM band_members \n",
    "INNER JOIN band_instruments\n",
    "ON band_members.name = band_instruments.name;\n",
    "\n",
    "-- Make it more readable with aliasing\n",
    "SELECT bm.name, band, plays\n",
    "FROM band_members AS bm\n",
    "INNER JOIN band_instruments AS bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "-- Note, we don't *need* the AS keyword\n",
    "-- Some RDBMSes actually won't let you use AS to alias tables\n",
    "SELECT bm.name, band, plays\n",
    "FROM band_members bm\n",
    "INNER JOIN band_instruments bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "-- Conversely we could prefix every column if we wanted\n",
    "-- This isn't needed since band & plays aren't ambiguous\n",
    "SELECT bm.name, bm.band, bi.plays\n",
    "FROM band_members AS bm\n",
    "INNER JOIN band_instruments AS bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "-- A FULL OUTER JOIN grabs every row from both tables\n",
    "-- You'll sometimes see OUTER dropped from this type of JOIN\n",
    "SELECT bm.name as bm_name, \n",
    "       bi.name as bi_name, \n",
    "\t   band,\n",
    "\t   plays\n",
    "FROM band_members AS bm\n",
    "FULL OUTER JOIN band_instruments AS bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "-- LEFT OUTER JOIN (think venn diagram)\n",
    "-- you'll often see people drop the OUTER from this \n",
    "-- type of JOIN\n",
    "SELECT *\n",
    "FROM band_members AS bm LEFT JOIN band_instruments AS bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "-- RIGHT JOINs work very similarly to LEFT JOINs\n",
    "SELECT *\n",
    "FROM band_members AS bm RIGHT JOIN band_instruments AS bi\n",
    "ON bm.name = bi.name;\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "-- Set operations ----------------\n",
    "----------------------------------\n",
    "\n",
    "-- Joins will combine tables horizontally combine\n",
    "-- tables, sometimes we have a usecase for combining\n",
    "-- tables vertically.  For this, we can use UNION.\n",
    "-- Using UNION will only grab unique records\n",
    "-- This can be thought of as set addition.\n",
    "-- What are some potential use cases of UNION?\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- pd.concat((df1, df2))\n",
    "SELECT *\n",
    "FROM band_members\n",
    "UNION\n",
    "SELECT *\n",
    "FROM band_members;\n",
    "\n",
    "-- Note, it doesn't check column names aligning\n",
    "SELECT band, name\n",
    "FROM band_members\n",
    "UNION\n",
    "SELECT *\n",
    "FROM band_instruments;\n",
    "\n",
    "\n",
    "-- UNION ALL will not filter out duplicates\n",
    "SELECT *\n",
    "FROM band_members\n",
    "UNION ALL\n",
    "SELECT *\n",
    "FROM band_members;\n",
    "\n",
    "\n",
    "-- datatype alignment\n",
    "SELECT CAST(1 AS CHAR) AS one, band\n",
    "FROM band_members\n",
    "UNION\n",
    "SELECT *\n",
    "FROM band_members;\n",
    "\n",
    "-- Using INTERSECT will stack tables similarly\n",
    "-- to UNION.  The difference is that INTERSECT\n",
    "-- only keeps rows common to both tables.\n",
    "-- What are some potential use cases of INTERSECT?\n",
    "SELECT *\n",
    "FROM band_members\n",
    "INTERSECT\n",
    "SELECT *\n",
    "FROM band_members\n",
    "WHERE name = 'John';\n",
    "\n",
    "\n",
    "-- The last set operator we're going to look at is\n",
    "-- EXCEPT.  This can be thought of as set subtraction.\n",
    "-- EXCEPT will give us all rows from the first table\n",
    "-- that are not present in the second.\n",
    "SELECT *\n",
    "FROM band_members\n",
    "EXCEPT\n",
    "SELECT *\n",
    "FROM band_members\n",
    "WHERE name = 'John';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                 btc_skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"bitcoinhistoricaldata\"\n",
    "\n",
    "\n",
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n",
    "df = pd.read_sql_query(\"SELECT * FROM coinbase\", con=conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(df[\"volume_btc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(df[\"volume_btc\"]).skewness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"volume_btc\"].hist()\n",
    "plt.title(\"raw volume_btc\")\n",
    "plt.show()\n",
    "\n",
    "np.log(df[\"volume_btc\"]).hist()\n",
    "plt.title(\"log(volume_btc)\")\n",
    "plt.show()\n",
    "\n",
    "np.sqrt(df[\"volume_btc\"]).hist()\n",
    "plt.title(\"sqrt(volume_btc)\")\n",
    "plt.show()\n",
    "\n",
    "(df[\"volume_btc\"] ** (1 / 3)).hist()\n",
    "plt.title(\"volume_btc ^ (1/3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         dates.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Using table: dvdrentals.public.rental\n",
    "\n",
    "-- We're cracking down on overdue rentals.\n",
    "-- We have little power over this so we're going to \n",
    "-- take a passive aggressive route.  For anyone,\n",
    "-- who has had their rental for over 1 week, label them with\n",
    "-- \"i'm lame and i turn things in late\". For those who've had\n",
    "-- their dvd for under 24 hours, label them with \"newbie\".\n",
    "-- Make up a label or leave the remaining unlabeled.\n",
    "\n",
    "-- Use age with only 1 arg to compare to today\n",
    "SELECT AGE(rental_date)\n",
    "FROM rental;\n",
    "\n",
    "-- Actual response to prompt\n",
    "SELECT customer_id,\n",
    "       rental_date,\n",
    "\t   CASE\n",
    "\t      WHEN AGE(rental_date) > '1 week' THEN 'i''m lame and i turn things in late'\n",
    "\t      WHEN AGE(rental_date) < '24 hours' THEN 'newbie'\n",
    "\t   END AS age_label\n",
    "FROM rental;\n",
    "\n",
    "\n",
    "-- Everyone knows Garfield hates Mondays.  Garfield also happens\n",
    "-- to manage this DVD rental place.  Everyone that rents DVDs on\n",
    "-- Mondays is dead to him.  Return all records where the rental \n",
    "-- date is not a Monday.  Include a column showing the day of the\n",
    "-- week in this output.\n",
    "-- reference: https://www.postgresql.org/docs/9.6/functions-formatting.html\n",
    "\n",
    "-- Day TO_CHAR examples:\n",
    "SELECT rental_date\n",
    "       ,TO_CHAR(rental_date, 'Day')\n",
    "       ,TO_CHAR(rental_date, 'DAY')\n",
    "\t   ,TO_CHAR(rental_date, 'day')\n",
    "\t   ,TO_CHAR(rental_date, 'D')\n",
    "\t   ,TO_CHAR(rental_date, 'DD')\n",
    "\t   ,TO_CHAR(rental_date, 'DDD')\n",
    "FROM rental;\n",
    "\n",
    "-- Prompt response:\n",
    "SELECT rental_date\n",
    "       ,TO_CHAR(rental_date, 'Day') AS weekday\n",
    "-- \t   ,'Lasagna' AS please_dont_fire_me\n",
    "FROM rental\n",
    "WHERE NOT TO_CHAR(rental_date, 'Day') = 'Monday';\n",
    "\n",
    "\n",
    "-- All of our rentals are due a week after their rental date.\n",
    "-- Create a column showing the due date for each rental.\n",
    "-- Create a column indicating whether or not the dvd is overdue.\n",
    "SELECT rental_date\n",
    "       ,rental_date + INTERVAL '1 week' AS due_date\n",
    "       ,CASE\n",
    "\t       WHEN AGE(rental_date) > '1 week' AND return_date IS NULL THEN 'Late'\n",
    "\t\t   ELSE 'Not Late'\n",
    "\t    END AS overdue\n",
    "FROM rental;\n",
    "\n",
    "\n",
    "-- INTERVAL will convert strings to a time interval data type\n",
    "-- not always needed, psql makes some good assumptions with this\n",
    "SELECT '1 week', INTERVAL '1 week';\n",
    "\n",
    "-- How long do people keep their rentals for?\n",
    "-- Show a list of everyone who had their rental over a week.\n",
    "SELECT return_date - rental_date AS rental_len\n",
    "FROM rental\n",
    "WHERE return_date - rental_date > '9 days';\n",
    "\n",
    "\n",
    "-- Almost all programmers will agree that the best \n",
    "-- way to write a date is YYYY-MM-DD.  However, your\n",
    "-- average US citizen might not agree. Let's malicously\n",
    "-- comply with society by showing the rental date as\n",
    "-- month_number/day_number/year and the return date as \n",
    "-- month_name day_number, year.\n",
    "-- reference: https://www.postgresql.org/docs/9.6/functions-formatting.html\n",
    "SELECT TO_CHAR(rental_date, 'MM/DD/YYYY') AS MM_DD_YYYY, \n",
    "       TO_CHAR(return_date, 'Month, DD, YYYY') AS Month_DD_YYYY\n",
    "FROM rental;\n",
    "\n",
    "\n",
    "SELECT CONCAT(\n",
    "\t          TRIM(TO_CHAR(return_date, 'Month')),\n",
    "\t          ' ',\n",
    "\t          TO_CHAR(return_date, 'DD'), \n",
    "\t          ', ', \n",
    "\t          TO_CHAR(return_date, 'YYYY')\n",
    "        ) AS date_str\n",
    "FROM rental;\n",
    "\n",
    "-- Make up a prompt to use different techniques we've\n",
    "-- used so far and bring it to workshop to challenge\n",
    "-- everyone.  It can use whatever database & table you'd like.\n",
    "\n",
    "\n",
    "-- Find the customer that has the largest total of days late\n",
    "-- the amount of days past the 1 week rental time\n",
    "-- include inventory_id\n",
    "SELECT AGE(return_date, rental_date) - '1 week' AS late_amount\n",
    "FROM rental\n",
    "WHERE AGE(return_date, rental_date) - '1 week' > '0 seconds'\n",
    "ORDER BY late_amount DESC\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      db_csv_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "user = \"postgres\"\n",
    "pwd = \"***********\" # Replace with your password\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "db = \"postgres\"\n",
    "\n",
    "engine = create_engine(\"postgresql://\" + user + \":\" + pwd + \n",
    "                       \"@\" + host + \":\" + port + \"/\" + db)\n",
    "\n",
    "print('downloading vehicles data...')\n",
    "vehicles = pd.read_csv(\"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/vehicles.csv\")\n",
    "print('loading vehicles data into db...')\n",
    "vehicles.to_sql(\"vehicles\", engine, index=False)\n",
    "\n",
    "print('downloading houseprices data...')\n",
    "houseprices = pd.read_csv(\"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/houseprices.csv\")\n",
    "print('loading houseprices data into db...')\n",
    "houseprices.to_sql(\"houseprices\", engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         dvdrental_joins.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Using the dvdrentals database:\n",
    "-- Return all matching records as a result of joining \n",
    "-- the payment and customer tables. Keep only records found in both tables.\n",
    "\n",
    "-- How do we think these tables join up?\n",
    "-- To prove out the foreign keyness of this field we \n",
    "-- could also view the table constraints.\n",
    "SELECT * FROM payment;\n",
    "SELECT * FROM customer;\n",
    "\n",
    "\n",
    "-- Lets now perform the join\n",
    "-- Reminder of the prompt: \n",
    "-- \"Keep only records found in both tables\"\n",
    "-- This query will return 2 copies of the `customer_id` field\n",
    "SELECT *\n",
    "FROM payment AS p\n",
    "INNER JOIN customer AS c\n",
    "ON p.customer_id = c.customer_id;\n",
    "\n",
    "\n",
    "-- Return the first name, last name and address of \n",
    "-- customers as a result of joining the customer and address tables. \n",
    "-- Keep only records found in both tables.\n",
    "SELECT \n",
    "\tfirst_name,\n",
    "\tlast_name,\n",
    "\taddress\n",
    "FROM customer AS cus\n",
    "\tJOIN address AS ad\n",
    "ON cus.address_id = ad.address_id;\n",
    "\n",
    "\n",
    "-- We can keep the JOIN ðŸš‚ rolling and join on join on join on join on join on join on join on join on join...\n",
    "-- Add payment amounts to the last query\n",
    "\n",
    "-- Note, this is introducing duplicate info from LEFT table\n",
    "-- Check row counts with COUNT(*) and note what the expected\n",
    "-- output row count is.\n",
    "SELECT \n",
    "\tfirst_name,\n",
    "\tlast_name,\n",
    "\taddress,\n",
    "\tamount\n",
    "FROM customer AS cus\n",
    "\tJOIN address AS ad\n",
    "ON cus.address_id = ad.address_id\n",
    "LEFT JOIN payment AS p\n",
    "ON cus.customer_id = p.customer_id;\n",
    "\n",
    "-- Checking expected output row count:\n",
    "SELECT COUNT(*)\n",
    "FROM customer AS cus\n",
    "\tJOIN address AS ad\n",
    "ON cus.address_id = ad.address_id;\n",
    "\n",
    "SELECT COUNT(*)\n",
    "FROM customer AS cus\n",
    "\tJOIN address AS ad\n",
    "ON cus.address_id = ad.address_id\n",
    "LEFT JOIN payment AS p\n",
    "ON cus.customer_id = p.customer_id;\n",
    "\n",
    "-- Let's 'solve' the duplication by showing total\n",
    "-- customer payments.\n",
    "SELECT \n",
    "\tcus.first_name,\n",
    "\tcus.last_name,\n",
    "\tad.address,\n",
    "\tSUM(p.amount)\n",
    "FROM customer AS cus\n",
    "\tJOIN address AS ad\n",
    "       ON cus.address_id = ad.address_id\n",
    "    LEFT JOIN payment AS p\n",
    "       ON cus.customer_id = p.customer_id\n",
    "GROUP BY cus.first_name, cus.last_name, ad.address;\n",
    "\n",
    "-- When joining many tables it can be useful to specify\n",
    "-- what table everything is coming from, even though it isn't necesary.\n",
    "-- If you hadn't already, add table specifiers before every\n",
    "-- column you mention in the SELECT clause\n",
    "\n",
    "\n",
    "\n",
    "-- We can also JOIN a table to itself\n",
    "-- Example potential use case:\n",
    "-- We have a 'people' table modeled like below.\n",
    "-- We could join it to itself to get a marriage per row.\n",
    "-- |  id  |  name  | married_to |\n",
    "-- |:----:|:------:|:----------:|\n",
    "-- |  1   | Skyler |     3      |\n",
    "-- |  2   | Elliot |    NULL    |\n",
    "-- |  3   | Peyton |     1      |\n",
    "\n",
    "-- SELECT *\n",
    "-- FROM people AS a\n",
    "--   JOIN people AS b\n",
    "--     ON a.id = b.married_to;\n",
    "\n",
    "-- Don't have a great setup for our dvdrentals data\n",
    "-- but this is how a self JOIN query could look like.\n",
    "-- This is also a use case for aliasing where its not\n",
    "-- only an aid to readability.\n",
    "SELECT a.customer_id,\n",
    "       CONCAT('A - ', a.first_name) AS cust_name_a,\n",
    "       CONCAT('B - ', b.first_name) AS cust_name_b\n",
    "FROM customer AS a\n",
    "INNER JOIN customer AS b\n",
    "ON a.customer_id = b.customer_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         intro_conditional_logic_alias_limit.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "-- INTRO QUERIES ---------------\n",
    "--------------------------------\n",
    "\n",
    "-- SELECT * (pronounced 'select star')\n",
    "-- is a 'wildcard' character meaning 'i want all the columns'\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me all the columns from the houseprices table\"\n",
    "SELECT *\n",
    "FROM houseprices;\n",
    "\n",
    "\n",
    "-- Sometimes you might see the schema name before the table name\n",
    "-- Think of how we call functions from pandas like pd.read_csv().\n",
    "-- This is unneeded in this case but sometimes you might have multiple\n",
    "-- schemas and this can allow you to reference 2 different tables with the same\n",
    "-- name in different schemas (i.e. schema1.prices & schema2.prices)\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me all the columns from the houseprices table\n",
    "-- located in the public schema\"\n",
    "SELECT *\n",
    "FROM public.houseprices;\n",
    "\n",
    "\n",
    "-- Sometimes you might see the database before the schema.\n",
    "-- This is the same as specifying the schema.  Not all RDBMSs\n",
    "-- allow you to query tables from multiple databases at once.\n",
    "-- Postgres does not allow querying multiple DBs in the same query by default.\n",
    "-- MS SQL Server is an example of an RDBMS that allows this.\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me all the columns from the houseprices table \n",
    "-- located in the public schema of the houseprices database\"\n",
    "SELECT *\n",
    "FROM houseprices.public.houseprices;\n",
    "\n",
    "\n",
    "-- If you want to be more specific, instead of using *, we\n",
    "-- can ask for fields by name.  The general SQL syntax is\n",
    "-- SELECT column1, column2\n",
    "-- FROM table\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me the yearbuilt and saleprice columns from the houseprices table\"\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices[['yearbuilt', 'saleprice']]\n",
    "SELECT yearbuilt, saleprice\n",
    "FROM houseprices;\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "-- EXPLORING A NEW TABLE -------\n",
    "--------------------------------\n",
    "\n",
    "-- When you first get your hands on a table, you'll want to explore.\n",
    "-- Ways to explore:\n",
    "--   * A schema diagram might be provided\n",
    "--   * Use a GUI (like pgAdmin) to click around and view fields/tables/schemas/DBs\n",
    "--   * Use the `information_schema`\n",
    "\n",
    "-- We'll talk more about all the parts of this query in a bit.\n",
    "-- For now, you might just keep a tab on this.\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me the column names and datatypes for the houseprices table\"\n",
    "SELECT column_name, data_type \n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'houseprices';\n",
    "\n",
    "\n",
    "-- query translation: \n",
    "-- \"give me the column names and datatypes for the transactions table\"\n",
    "SELECT column_name, data_type \n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'transactions';\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "-- FILTERING WITH WHERE --------\n",
    "--------------------------------\n",
    "\n",
    "-- Very commonly, we want only a subset of records based on a \n",
    "-- condition.  For this, we'll use the WHERE clause.  Our new\n",
    "-- template for including this will be:\n",
    "-- SELECT column(s)\n",
    "-- FROM table\n",
    "-- WHERE condition(s)\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[houseprices['yearbuilt'] > 1984, ['yearbuilt', 'saleprice']]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt > 1984;\n",
    "\n",
    "\n",
    "-- For equality checks, SQL uses single `=` not `==` like in python\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[houseprices['yearbuilt'] == 1984, ['yearbuilt', 'saleprice']]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt = 1984;\n",
    "\n",
    "\n",
    "-- In postgreSQL both `<>` and `!=` work for not equal\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[houseprices['yearbuilt'] != 1984, ['yearbuilt', 'saleprice']]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt <> 1984;\n",
    "\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt != 1984;\n",
    "\n",
    "\n",
    "-- Logical operators\n",
    "-- AND/OR/NOT like you'd expect with a pythonic mindset\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[\n",
    "--     (houseprices[\"yearbuilt\"] > 1984) & (houseprices[\"saleprice\"] > 300000),\n",
    "--     [\"yearbuilt\", \"saleprice\"],\n",
    "-- ]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt > 1984 AND saleprice > 300000;\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[\n",
    "--     (houseprices[\"yearbuilt\"] > 1984) | (houseprices[\"saleprice\"] > 300000),\n",
    "--     [\"yearbuilt\", \"saleprice\"],\n",
    "-- ]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt > 1984 OR saleprice > 300000;\n",
    "\n",
    "\n",
    "-- IN also works similarly to in python\n",
    "\n",
    "-- pandas equivalent (base python would use `in`, pandas uses `.isin()`):\n",
    "-- houseprices.loc[\n",
    "--     houseprices[\"yearbuilt\"].isin([1985, 1995, 2005]), \n",
    "--     [\"yearbuilt\", \"saleprice\"]\n",
    "-- ]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt IN (1985, 1995, 2005);\n",
    "\n",
    "\n",
    "-- BETWEEN can be useful to filter to a range (inclusive)\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices.loc[\n",
    "--     houseprices[\"yearbuilt\"].between(1984, 1985), \n",
    "--     [\"yearbuilt\", \"saleprice\"]\n",
    "-- ]\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt BETWEEN 1984 AND 1985;\n",
    "\n",
    "-- Equivalent written out with traditional comparison operators\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "WHERE yearbuilt >= 1984 AND yearbuilt <= 1985;\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "-- CASE STATEMENTS -------------\n",
    "--------------------------------\n",
    "\n",
    "-- We might want to create a field in our output based on some conditions\n",
    "-- For example, maybe we want to label home prices as above or below a value.\n",
    "-- According to zillow (as of 2020-07-27): \n",
    "-- \"The median home value in the United States is $248,857.\"\n",
    "-- We can use the same conditional logic as in the WHERE statement to create\n",
    "-- a label column.\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- df = houseprices.loc[:, [\"saleprice\", \"yearbuilt\"]]\n",
    "-- df[\"case\"] = \"Below Median\"\n",
    "-- df.loc[df[\"saleprice\"] > 248857, \"case\"] = \"Above Median\"\n",
    "-- df.loc[df[\"saleprice\"] == 248857, \"case\"] = \"Median\"\n",
    "SELECT saleprice, yearbuilt,\n",
    "       CASE \n",
    "\t     WHEN saleprice > 248857 THEN 'Above Median'\n",
    "\t\t WHEN saleprice = 248857 THEN 'Median'\n",
    "\t     ELSE 'Below Median'\n",
    "\t   END\n",
    "FROM houseprices;\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "-- ALIASING COLUMNS ------------\n",
    "--------------------------------\n",
    "\n",
    "-- Sometimes we want to rename a column in our output\n",
    "-- This does not change the field name in the database, just in our output\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- df = houseprices.loc[:, [\"saleprice\", \"yearbuilt\"]]\n",
    "-- df = df.rename(columns={\"yearbuilt\": \"yb\"})\n",
    "SELECT saleprice, yearbuilt AS yb\n",
    "FROM houseprices;\n",
    "\n",
    "-- Note that we cannot use this alias in the WHERE statement\n",
    "-- We'll use aliasing more in the future and experiment with\n",
    "-- where we can/can't use these aliases.\n",
    "SELECT saleprice, yearbuilt AS yb\n",
    "FROM houseprices\n",
    "WHERE yb > 1990;\n",
    "\n",
    "\n",
    "--------------------------------\n",
    "-- LIMITING QUERY SIZE ---------\n",
    "--------------------------------\n",
    "\n",
    "-- LIMIT works like `pandas.DataFrame.head()`\n",
    "-- It allows you to SELECT the top n rows FROM a table\n",
    "-- As we'll see, this pairs well with sorting to be able\n",
    "-- to query for the top n based on some field.\n",
    "\n",
    "-- pandas equivalent:\n",
    "-- houseprices[['saleprice', 'yearbuilt']].head(2)\n",
    "SELECT saleprice, yearbuilt\n",
    "FROM houseprices\n",
    "LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    math_operations.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To answer these use the table:\n",
    "-- bitcoinhistoricaldata.public.coinbase\n",
    "\n",
    "-- Request from business client: \n",
    "--   Show me the days that had gaps greater than 80 between open & close.\n",
    "--   Info wanted for these days:\n",
    "--     * What were the dates?\n",
    "--     * How large are these gaps?\n",
    "--     * What is the largest gap?\n",
    "--     * What were the open/close values?\n",
    "SELECT timestamp, \n",
    "       ABS(open - close) AS gap, \n",
    "       open, \n",
    "       close \n",
    "FROM   coinbase \n",
    "WHERE  ABS(open - close) > 85; \n",
    "\n",
    "\n",
    "-- Request from business client: \n",
    "--   I'm a superstitious trader and I'd only like to trade on days where the open\n",
    "--   value is a multiple of 5.  Are there days in the table where I'd be \n",
    "--   able to trade?  Can you please send me a csv of the records where my\n",
    "--   superstition would allow me to trade?\n",
    "SELECT * \n",
    "FROM   coinbase \n",
    "WHERE  open :: DECIMAL % 5 = 0; \n",
    "\n",
    "SELECT * \n",
    "FROM   coinbase \n",
    "WHERE  CAST(open AS DECIMAL) % 5 = 0; \n",
    "\n",
    "\n",
    "-- The `volume_btc` field is fairly postively skewed.\n",
    "-- That is most values are low but some are very high (see btc_skew.ipynb for more).\n",
    "-- One way of dealing with this skew in machine learning\n",
    "-- is by using transformations.  Some possible transformations\n",
    "-- are log, square root, and cube root.  Write a query to return:\n",
    "--   * every field in the table and \n",
    "--   * a field for each of these transformations\n",
    "SELECT *, \n",
    "       LOG(volume_btc)            AS log_vol_btc, \n",
    "       SQRT(volume_btc)           AS sqrt_vol_btc, \n",
    "       volume_btc ^ ( 1.0 / 3.0 ) AS cbrt_vol_btc \n",
    "FROM   coinbase; \n",
    "\t   \n",
    "\n",
    "-- Request from business client:\n",
    "--   I want an instability metric thats a single value to show how volatile\n",
    "--   the market was on a given day.  I'd like this metric to be the weighted \n",
    "--   average between:\n",
    "--     * The size of the gap between high/low   (40% weight)\n",
    "--     * The size of the gap between open/close (60% weight)\n",
    "--   I'd like this number to be rounded to only show one decimal.\n",
    "--   Only show me the numbers where the open is different than the close.\n",
    "--   If you think of a better metric, let me know, but please deliver on this\n",
    "--   request before suggesting alternatives.\n",
    "SELECT *, \n",
    "       -- ROUND(what_to_round, number_of_decimal_places) \n",
    "       ROUND(( ( high - low ) * 0.4 + ABS(open - close) * 0.6 ) :: DECIMAL, 1) AS metric_wt, \n",
    "       -- Round up to nearest int (no option on number of decimals)\n",
    "       CEIL(( ( high - low ) * 0.4 + ABS(open - close) * 0.6 ) :: DECIMAL) AS metric_wt_ceil, \n",
    "       -- Round down to nearest int (no option on number of decimals)\n",
    "       FLOOR(( ( high - low ) * 0.4 + ABS(open - close) * 0.6 ) :: DECIMAL) AS metric_wt_floor \n",
    "FROM   coinbase \n",
    "WHERE  NOT open = close; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                       review_qa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makes = [\"Ford\", \"Chevrolet\", \"Toyota\", \"Honda\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"fueleconomy\"\n",
    "\n",
    "\n",
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query_template = \"\"\"\n",
    "SELECT make,\n",
    "       model, \n",
    "       year,\n",
    "       hwy\n",
    "FROM vehicles\n",
    "WHERE make = '{}'\n",
    "ORDER BY hwy DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(query_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine = create_engine(conn_str)\n",
    "\n",
    "dfs = []\n",
    "for make in makes:\n",
    "    query = query_template.format(make)\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    dfs.append(df)\n",
    "\n",
    "engine.dispose()\n",
    "\n",
    "full_df = pd.concat(dfs)\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fueleconomy.public.vehicles\n",
    "\n",
    "    # prompt:\n",
    "    # Write a query to return every vehicle with above average cty mpg.\n",
    "    # Include in the output a column labeling whether the hwy mpg is\n",
    "    # above/below avg.\n",
    "\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"fueleconomy\"\n",
    "\n",
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n",
    "\n",
    "    # potential solution\n",
    "query = \"\"\"\n",
    "SELECT make,\n",
    "       model,\n",
    "       CASE\n",
    "         WHEN hwy >= (SELECT AVG(hwy) FROM vehicles) THEN 'Above Avg Hwy'\n",
    "         ELSE 'Below Avg Hwy'\n",
    "       END AS hwy_rating\n",
    "FROM vehicles\n",
    "WHERE cty > (SELECT AVG(cty) FROM vehicles);\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential structure for housing multiple queries in a project repo:\n",
    "\n",
    "#.\n",
    "        # â”œâ”€â”€ README.md\n",
    "    # â””â”€â”€ queries\n",
    "    #     â”œâ”€â”€ task1.sql\n",
    "    #     â””â”€â”€ task2.sql\n",
    "\n",
    "    # glob allows for pattern matching file names\n",
    "import glob\n",
    "\n",
    "    # You know what you want\n",
    "sql_files = [\"task1.sql\", \"task2.sql\"]\n",
    "\n",
    "    # You dont know and want the os to tell you\n",
    "    # pattern here is saying:\n",
    "    # \"match anything that ends in .sql in current dir\"\n",
    "sql_files = glob.glob(\"*.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH answer AS (\n",
    "    SELECT concat(namelast,', ', namefirst) AS namefull, yearid, inducted\n",
    "    FROM hof_inducted LEFT OUTER JOIN people\n",
    "    ON hof_inducted.playerid = people.playerid\n",
    "    WHERE yearid >= 1980\n",
    "\n",
    "    UNION ALL\n",
    "    --UNION\n",
    "\n",
    "    SELECT concat(namelast,', ', namefirst) AS namefull, yearid, inducted\n",
    "    FROM hof_not_inducted LEFT OUTER JOIN people\n",
    "    ON hof_not_inducted.playerid = people.playerid\n",
    "    WHERE yearid >= 1980\n",
    "\n",
    "    ORDER BY yearid,  inducted DESC, namefull\n",
    ")\n",
    "\n",
    "SELECT COUNT(*)\n",
    "FROM answer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    class_notebooks/week2/sql_in_python.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Installations for talking to postgres\n",
    "    # !pip install psycopg2-binary\n",
    "    # !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note, in practice, storing a password as a string in code is bad.\n",
    "    # In practice, you might read this password from a secret manager, or at\n",
    "    # a minimum store it in a different file that is listed in your `.gitignore`.\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"medicalcosts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Define connection string\n",
    "conn_str = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db\n",
    ")\n",
    "conn_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine = create_engine(conn_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"SELECT * FROM medicalcosts WHERE age > 30\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM medicalcosts\n",
    "WHERE age > 30\n",
    "\"\"\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"math_operations.sql\", \"r\") as f:\n",
    "    q = f.read()\n",
    "\n",
    "    # print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "med_costs = pd.read_sql_query(query, engine)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"medicalcosts\"\n",
    "\n",
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM medicalcosts\n",
    "LIMIT 5000\n",
    "\"\"\"\n",
    "\n",
    "medical_costs = pd.read_sql_query(query, con=conn_str)\n",
    "medical_costs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Read contents of `secrets.json` to a dictionary\n",
    "import json\n",
    "\n",
    "with open(\"../secrets.json\", \"r\") as f:\n",
    "    secrets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postgres_user = \"postgres\"\n",
    "postgres_pw = secrets[\"local_psql_pw\"]\n",
    "postgres_host = \"localhost\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"postgres\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine = create_engine(conn_str)\n",
    "engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #dir(engine)\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "meta = MetaData(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vehicles = Table(\"vehicles\", meta, autoload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(vehicles.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # For writing data to SQL from pandas we can use a dataframe's `.to_sql()` method.\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.to_sql(\"iris\", engine, index=True)\n",
    "\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # To execture arbitrary sql we can use a sqlalchemy engine's `.execute()` method.\n",
    "query = \"DROP TABLE iris\"\n",
    "engine.execute(query)\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  class_notebooks/week2/string_operations.sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer these use the table:\n",
    "-- baseball.public.people\n",
    "\n",
    "-- Request from client:\n",
    "--    We're going to be interviewing players and we want\n",
    "--    some text to put on screen so viewers know who they are.\n",
    "--    Can you create 2 text fields for this? (1 for each line we'll show on screen)\n",
    "--     * The first text field should have the players name and their age like:\n",
    "--          \"David Aardsma (age: 31)\"\n",
    "--     * The second line should have the handedness:\n",
    "--          \"Bats: R; Throws: R\"\n",
    "--  Please name these fields to indicate which is line 1 and line 2.\n",
    "--  We're only going to use this overlay for 2020, so there's no need to future proof.\n",
    "SELECT namefirst || ' ' || namelast || ' (age: ' || 2020 -  birthyear || ')' AS line_1,\n",
    "       'Bats: ' || bats || '; Throws: ' || throws AS line_2\n",
    "FROM people;\n",
    "\n",
    "-- will also work here\n",
    "SELECT CONCAT(namefirst, ' ', namelast, ' (age: ', 2020 -  birthyear, ')') AS line_1,\n",
    "       CONCAT('Bats: ', bats, '; Throws: ', throws) AS line_2\n",
    "FROM people;\n",
    "\n",
    "-- will work in ms sql server\n",
    "-- SELECT namefirst + ' ' + namelast + '(age: ' + 2020 -  birthyear + ')'\n",
    "-- FROM people;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- Request from client:\n",
    "--  I can't explain it, but our new CEO says we need to show the players\n",
    "--  names starting with the letter B.  Can you write a query to output the \n",
    "--  players' first and last names with the first letter of each replaced with a B?\n",
    "--  Name these outputs bamefirst and bamelast.\n",
    "\n",
    "-- SUBSTRING(column, start_position, length)\n",
    "SELECT 'B' || SUBSTRING(namefirst, 2) AS bamefirst,\n",
    "       'B' || SUBSTRING(namelast, 2) AS bamelast\n",
    "FROM people;\n",
    "\n",
    "\n",
    "-- Request from client:\n",
    "--  Ok, that last CEO didn't last long.... \n",
    "--  after the B fiasco they were replaced.\n",
    "--  The new CEO wants to go the other direction.  \n",
    "--  Can you remove every B from the player names (case insensitive, all bs MUST go)\n",
    "--  Might be an overcorrection but Â¯\\_(ãƒ„)_/Â¯...\n",
    "--  To ensure that this is working, will you output both the original\n",
    "--  names and the 'cleaned' names.  Additionally, filter the output\n",
    "--  to only show names that orginally had Bs in them.\n",
    "SELECT namefirst,\n",
    "       namelast,\n",
    "       REPLACE(REPLACE(namefirst, 'B', ''), 'b', '') AS no_b_first,\n",
    "       REPLACE(REPLACE(namelast, 'B', ''), 'b', '') AS no_b_last\n",
    "FROM people\n",
    "-- '%b%' find everything with a 'b' in it\n",
    "WHERE namefirst ILIKE '%b%' OR namelast ILIKE '%b%';\n",
    "\n",
    "-- Will work on p much any flavor\n",
    "SELECT namefirst, namelast\n",
    "FROM people\n",
    "WHERE LOWER(namefirst) LIKE '%b%';\n",
    "\n",
    "\n",
    "-- Find where b is the 3rd character\n",
    "SELECT namefirst, namelast\n",
    "FROM people\n",
    "WHERE namefirst LIKE '__b%';\n",
    "\n",
    "-- ok... no more made up context to give. so this ones real.\n",
    "-- i think right handed people are better;\n",
    "-- to reflect this, put the first/last names of players\n",
    "-- who bat and throw right handed in all caps. anyone who uses\n",
    "-- their left hand for anything should have their names written in all lower case.\n",
    "-- please provide a query to assert right hand dominance\n",
    "-- (return the bats & throws so i can double check the work)\n",
    "-- for an extra challenge, right this query with only your right hand\n",
    "SELECT bats, \n",
    "       throws,\n",
    "\t   CASE\n",
    "\t\t  WHEN bats = 'R' AND throws = 'R' THEN UPPER(CONCAT(namefirst, ' ', namelast))\n",
    "\t\t  ELSE LOWER(CONCAT(namefirst, ' ',namelast))\n",
    "\t   END AS disp\n",
    "FROM people;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                               class_notebooks/week2/subquery_cte_blank.sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Using the fueleconomy database\n",
    "\n",
    "-- Subqueries!!\n",
    "-- Find the average highway mileage for all records of \n",
    "-- whatever make produces the model 'A4'.\n",
    "\n",
    "-- Part 1 of answering this question: \n",
    "-- Who makes 'A4'?\n",
    "\n",
    "\n",
    "-- Part 2: filter\n",
    "\n",
    "\n",
    "-- A subquery can allow us to do both of this parts\n",
    "-- at once.\n",
    "\n",
    "\n",
    "\n",
    "-- Subqueries are not limited to the WHERE clause\n",
    "-- Say we want to compare each car to the average \n",
    "-- hwy mpg. Write a query to grab the model, hwy, \n",
    "-- and a column showing how far above/below the car's\n",
    "-- hwy is from average.\n",
    "\n",
    "\n",
    "\n",
    "-- Perform the same task as above, but instead\n",
    "-- of comparing to the overall average hwy, compare\n",
    "-- to the average for the current records make.\n",
    "-- (i.e. a bmw row in the output should be compared to\n",
    "-- to the average bmw hwy mpg)\n",
    "\n",
    "\n",
    "\n",
    "-- Write a query to only select records with \n",
    "-- above average hwy.\n",
    "\n",
    "\n",
    "\t\t\t \n",
    "-- Let's switch focus to makes.  Show the average\n",
    "-- hwy mpg by make, but only show makes who have\n",
    "-- average hwy mpg greater than the average hwy\n",
    "-- mpg in the vehicles table.\n",
    "\n",
    "\n",
    "\n",
    "-- Using SQL and the vehicles table from the fueleconomy database, do the following:\n",
    "-- Find the average of minimum highway mileages for records when grouped by make.\n",
    "-- what... lets unpack it\n",
    "-- Find the min hwy mileage per make and then take the average of it\n",
    "\n",
    "-- Let's use a CTE for this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                       class_notebooks/week3/ab_testing.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.stats.proportion as proportion\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_95_ci(x1, x2):\n",
    "    \"\"\"Calculate a 95% CI for 2 1d numpy arrays\"\"\"\n",
    "    signal = x1.mean() - x2.mean()\n",
    "    noise = np.sqrt(x1.var() / x1.size + x2.var() / x2.size)\n",
    "\n",
    "    ci_lo = signal - 1.96 * noise\n",
    "    ci_hi = signal + 1.96 * noise\n",
    "\n",
    "    return ci_lo, ci_hi\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Below package is needed for pandas to read xl files\n",
    "    # !pip install xlrd\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/a-b-testing-drill-start-06-14-19.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_excel(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.shape\n",
    "df.dtypes\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"group\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.groupby(\"group\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"cart_amount\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Overall conversion rate (see below for by group)\n",
    "df[\"convert\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treatment = df[df[\"group\"] == \"treatment\"]\n",
    "control = df[df[\"group\"] == \"control\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Assumptions\n",
    "    # -------------\n",
    "    # Continuous?      yep\n",
    "    # Independent?     think so   \n",
    "    # Random sample?   think so\n",
    "    # Normal?          yep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "treatment[\"cart_amount\"].hist()\n",
    "plt.show()\n",
    "\n",
    "stats.describe(treatment[\"cart_amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t, p = stats.ttest_ind(treatment[\"cart_amount\"], control[\"cart_amount\"])\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"sig diff; reject null\")\n",
    "else:\n",
    "    print(\"no sig diff; fail to reject null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # x1 - x2\n",
    "get_95_ci(treatment[\"cart_amount\"], control[\"cart_amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.groupby(\"group\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sns.boxplot(\"group\", \"cart_amount\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Assumptions\n",
    "    # -------------\n",
    "    # Continuous?      nope, binary\n",
    "    # Independent?     yep, think so\n",
    "    # Random sample?   yep, think so\n",
    "    # Normal?          nope, binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treatment[\"convert\"].hist()\n",
    "plt.show()\n",
    "\n",
    "stats.describe(treatment[\"convert\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.crosstab(df[\"group\"], df[\"convert\"], margins=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agg_df = df.groupby(\"group\").agg({\"convert\": [\"size\", \"sum\"]})\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "count = agg_df.iloc[:, 1]\n",
    "nobs = agg_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z, p_value = proportion.proportions_ztest(count, nobs)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"sig; reject null\")\n",
    "else:\n",
    "    print(\"no sig; fail to reject null\")\n",
    "    \n",
    "    # Having a testimonial does not appear to have an effect\n",
    "    # on conversion rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "chi2, p_value, expected = proportion.proportions_chisquare(count, nobs)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # count of people, count of converts, conversion rate\n",
    "df.groupby(\"group\").agg({\"convert\": [\"size\", \"sum\", \"mean\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "control_converts = control[\"convert\"].sum()\n",
    "control_n = control[\"convert\"].size\n",
    "control_rate = control[\"convert\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treatment_converts = treatment[\"convert\"].sum()\n",
    "treatment_n = treatment[\"convert\"].size\n",
    "treatment_rate = treatment[\"convert\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Calculate coordinates to plot the corresponding binomial dist\n",
    "\n",
    "    # Create a range of potential number of conversions\n",
    "    # Observed number plus/minus a buffer\n",
    "x = range(control_converts - 50, control_converts + 50)\n",
    "\n",
    "    # This needs as arguments:\n",
    "    #    * nobs for control\n",
    "    #    * conversion rate for control\n",
    "y = stats.binom(control_n, control_rate).pmf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # If the treatment had the same number of people.\n",
    "    # Calculate what the number of conversions would be\n",
    "    # with the treatment's conversion rate.\n",
    "treatment_converts = control_n * treatment_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Plot x by y and give this a label of 'Control'\n",
    "plt.plot(x, y, label=\"Control\")\n",
    "\n",
    "    # Add a vertical line where where the number\n",
    "    # of conversions would be if we had the treatment\n",
    "    # conversion rate. Label as treatment\n",
    "plt.axvline(treatment_converts, label=\"Treatment\", c=\"orange\")\n",
    "\n",
    "plt.xlabel(\"N Converted\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # pmf is probability mass function\n",
    "    # Answers question: whats the probability of obtaining a value at this given point?\n",
    "\n",
    "    # This is a point estimate of the probability of seeing the\n",
    "    # treatment number of conversions, assuming the rates are the \n",
    "    # same between treatment and control.\n",
    "\n",
    "    # According to this, there is over a 5% chance of seeing the treatment\n",
    "    # conversion rate (assuming the rate is equal to the control.)\n",
    "stats.binom(control_n, control_rate).pmf(int(treatment_converts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # cdf is cumalitve density function\n",
    "    # Answers question: whats the probability of obtaining a value at this given point or smaller?\n",
    "\n",
    "    # It differs from pmf, in that it sums up all the area under the distribution's\n",
    "    # curve.\n",
    "\n",
    "    # In our exercise we might use 1 - cdf to indicate, what the probability of \n",
    "    # obtaining a value at this point or greater.\n",
    "\n",
    "    # In context, assuming the rates of control and treatment are equal, then we would\n",
    "    # see the number of treatment conversions (or greater) 28% of the time.  In other\n",
    "    # words this isn't that interesting of a result.  It is not significantly different.\n",
    "1 - stats.binom(control_n, control_rate).cdf(int(treatment_converts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           class_notebooks/week3/anova_and_paired_t.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "    # fmt: off\n",
    "audio_book = np.array([7.5, 4. , 4. , 3. , 6.5, 1. , 8. , 4. , 5.5, 4.5, 7.5, 1.5, 4.5,\n",
    "                       4.5, 7. , 3. , 4.5, 3.5, 5. , 6. , 3. , 7. , 6.5, 6. , 6.5, 4. ,\n",
    "                       5. , 3.5, 4.5, 6. , 4. , 4.5, 4. , 3.5, 4. , 5. , 3. , 5.5, 8. ,\n",
    "                       6.5, 4.5, 3.5, 4. , 8. , 5. , 4. , 5.5, 8.5, 5. , 6. , 5.5, 4.5,\n",
    "                       3. , 4.5, 4.5, 6. , 6.5, 6.5, 5.5, 6.5, 4. , 7. , 6. , 4.5, 6. ,\n",
    "                       5. , 7. , 7.5, 8.5, 2.5, 2.5, 4. , 5.5, 6.5, 5.5, 1.5, 4.5, 6.5,\n",
    "                       5.5, 6.5, 4.5, 4.5, 5.5, 5.5, 5.5, 5. , 4. , 5.5, 5. , 7. , 7. ,\n",
    "                       5.5, 4.5, 4. , 5.5, 5. , 4.5, 5. , 4. , 6. ])\n",
    "\n",
    "\n",
    "control = np.array([ 6. ,  8.5,  7.5,  7.5,  5. ,  7. ,  8. ,  5. ,  6. ,  6.5,  4.5,\n",
    "                    7. ,  8. ,  5.5,  7.5,  4.5,  6.5,  4. ,  8.5,  7.5,  6.5,  5.5,\n",
    "                    9. , 10. ,  3.5,  9. ,  9.5,  7.5,  4.5,  8. ,  6.5,  5.5,  4.5,\n",
    "                    7.5,  8. ,  5.5,  7.5,  5. ,  8. ,  7. ,  6.5,  6.5,  8. ,  8. ,\n",
    "                    7.5,  7. ,  7. ,  7.5,  7. ,  8. ,  6. ,  2.5,  8.5, 10. ,  7.5,\n",
    "                    6.5,  6.5,  6.5,  6.5,  5. ,  6. ,  5. ,  7. ,  6. ,  7.5,  6.5,\n",
    "                    8.5,  7. , 10. ,  3.5,  5.5,  8. , 10. ,  6.5,  7. ,  6.5,  9. ,\n",
    "                    6. ,  8. ,  6. ,  4.5,  7. ,  7.5,  9. ,  6.5,  5.5,  7.5,  7. ,\n",
    "                    7. ,  6.5,  8.5,  7.5, 10. ,  8.5,  8. ,  4. ,  7.5,  7.5,  8. ,\n",
    "                    8.5])\n",
    "\n",
    "\n",
    "music = np.array([4. , 6. , 3.5, 8. , 2.5, 1.5, 2. , 3. , 3. , 8. , 4. , 2.5, 6.5,\n",
    "                  3.5, 2.5, 6. , 5.5, 4. , 7. , 5. , 7.5, 5.5, 7. , 5. , 2.5, 8.5,\n",
    "                  3.5, 4.5, 3.5, 4.5, 5. , 5. , 5.5, 7.5, 4.5, 2.5, 2.5, 3.5, 2. ,\n",
    "                  6. , 4. , 4. , 4.5, 2.5, 3. , 5.5, 5.5, 4. , 7.5, 2.5, 4. , 4. ,\n",
    "                  0. , 8. , 5. , 3. , 8. , 5.5, 5.5, 3.5, 5. , 2. , 6. , 4. , 5. ,\n",
    "                  6.5, 5. , 5. , 5. , 3.5, 6. , 7. , 6. , 6. , 3. , 3. , 2. , 6.5,\n",
    "                  4. , 7. , 3.5, 4. , 5. , 6. , 4.5, 5. , 4.5, 5. , 3. , 5. , 4.5,\n",
    "                  4.5, 5.5, 3.5, 4.5, 4. , 5.5, 6. , 3. , 5.5])\n",
    "    # fmt: on\n",
    "\n",
    "pain = pd.DataFrame({\"audio_book\": audio_book, \"music\": music, \"control\": control,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pain.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pain.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pain.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pain.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Google: \"pandas convert data from wide to tall\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tall_pain = pd.melt(pain, var_name=\"treatment\", value_name=\"pain\")\n",
    "tall_pain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # potential plot types\n",
    "    #    * [x] boxplot\n",
    "    #    * [ ] violinplot\n",
    "    #    * [x] histograms\n",
    "    #    * [x] swarmplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tall_pain.boxplot(\"pain\", by=\"treatment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tall_pain[\"pain\"].hist(by=tall_pain[\"treatment\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.swarmplot(\"treatment\", \"pain\", data=tall_pain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.shapiro(music)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.shapiro(audio_book)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.shapiro(control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(audio_book, line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(music, line=\"s\")\n",
    "plt.show()\n",
    "\n",
    "qqplot(control, line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tall_pain.groupby(\"treatment\").std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Check the docs:\n",
    "    # stats.bartlett?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.bartlett(audio_book, music, control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot(audio_book)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot(music)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f, p = stats.f_oneway(music, audio_book, control)\n",
    "f  # this is ratio of - between_variance / within_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lm = ols(\"pain ~ C(treatment)\", data=tall_pain).fit()\n",
    "anova_table = sm.stats.anova_lm(lm, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(lm.resid, line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.shapiro(lm.resid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tukey_results = pairwise_tukeyhsd(tall_pain[\"pain\"], tall_pain[\"treatment\"])\n",
    "\n",
    "    # Plot results\n",
    "tukey_results.plot_simultaneous()\n",
    "plt.axvline(4.859)\n",
    "plt.show()\n",
    "\n",
    "    # Show summary table of results\n",
    "tukey_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paired_pain = pain[[\"audio_book\", \"control\"]]\n",
    "paired_pain[\"diff\"] = pain[\"audio_book\"] - pain[\"control\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paired_pain[\"diff\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(paired_pain[\"diff\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "d = paired_pain[\"diff\"]\n",
    "\n",
    "signal = d.mean()\n",
    "noise = d.std() / np.sqrt(d.size)\n",
    "\n",
    "t = signal / noise\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t, p = stats.ttest_rel(paired_pain[\"audio_book\"], paired_pain[\"control\"])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ci_lo = signal - 1.96 * noise\n",
    "ci_hi = signal + 1.96 * noise\n",
    "\n",
    "ci_lo, ci_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pain[\"patient_id\"] = pain.index\n",
    "pain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tall_pain = pd.melt(pain, id_vars=\"patient_id\", var_name=\"treatment\", value_name=\"pain\")\n",
    "tall_pain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "anova = AnovaRM(\n",
    "    tall_pain, depvar=\"pain\", subject=\"patient_id\", within=[\"treatment\"]\n",
    ").fit()\n",
    "\n",
    "anova.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            class_notebooks/week3/anscombe.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/anscombe.csv\"\n",
    "abscombe = pd.read_csv(data_url)\n",
    "abscombe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abscombe.groupby(\"category\").describe().round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abscombe.plot(x=\"x\", y=\"y\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.scatterplot(x=\"x\", y=\"y\", data=abscombe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "px.scatter(abscombe, x=\"x\", y=\"y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "color_map = {\"a\": \"blue\", \"b\": \"orange\", \"c\": \"green\", \"d\": \"red\"}\n",
    "colors = abscombe[\"category\"].replace(color_map)\n",
    "\n",
    "abscombe.plot(x=\"x\", y=\"y\", color=colors, kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.scatterplot(x=\"x\", y=\"y\", hue=\"category\", data=abscombe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "px.scatter(abscombe, x=\"x\", y=\"y\", color=\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abscombe.groupby(\"category\").plot(x=\"x\", y=\"y\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Throwing in color too cause why not\n",
    "fg = sns.FacetGrid(data=abscombe, col=\"category\", hue=\"category\")\n",
    "fg.map(plt.scatter, \"x\", \"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Throwing in color too cause why not\n",
    "px.scatter(abscombe, x='x', y='y', facet_col='category', color='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.plotting.scatter_matrix(abscombe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.pairplot(abscombe)\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(abscombe, hue='category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = px.scatter_matrix(abscombe)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter_matrix(abscombe, color='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris = iris[[\"sepal_length\", \"petal_length\"]]\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris[\"large_sepal\"] = iris[\"sepal_length\"] > iris[\"sepal_length\"].median()\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f'a large sepal was defined as being greater than {iris[\"sepal_length\"].median()}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                class_notebooks/week3/marble_race.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _gen_player_data(color, name=\"\", y=0, n_steps=20):\n",
    "    \"\"\"Generate a random race time and split it out over time steps for plotting\"\"\"\n",
    "    # Generate random race times\n",
    "    finish_time = np.random.normal(n_steps * 0.8, 0.3)\n",
    "\n",
    "    # Find x position for each frame of race\n",
    "    rate = 1 / finish_time\n",
    "    step = finish_time / n_steps\n",
    "    time_steps = np.arange(n_steps + 1)\n",
    "    x_pos = time_steps * rate\n",
    "\n",
    "    # Store all plotting info for plotly\n",
    "    race_df = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": time_steps,\n",
    "            \"x\": x_pos,\n",
    "            \"y\": y,\n",
    "            \"color\": color,\n",
    "            \"name\": name,\n",
    "            \"finish_time\": finish_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add a little jitter to be less boring\n",
    "    excitement = np.ones_like(x_pos) * 0.01\n",
    "    excitement[: len(excitement) // 2] *= -1\n",
    "    np.random.shuffle(excitement)\n",
    "    race_df[\"x\"] += excitement\n",
    "    race_df.loc[0, \"x\"] = 0\n",
    "\n",
    "    return race_df\n",
    "\n",
    "\n",
    "def _gen_race_data(players, colors=px.colors.qualitative.T10):\n",
    "    \"\"\"'Simulate' a marble race between players\"\"\"\n",
    "    race_dfs = []\n",
    "    name_colors = zip(players, colors)\n",
    "    for i, (name, color) in enumerate(name_colors):\n",
    "        race_df = _gen_player_data(color, name, i * 0.1)\n",
    "        race_dfs.append(race_df)\n",
    "\n",
    "    return pd.concat(race_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def marble_race(players, seed=None):\n",
    "    \"\"\"'Simulate' a marble race\"\"\"\n",
    "    if isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "    race_df = _gen_race_data(players)\n",
    "\n",
    "    return (\n",
    "        race_df[[\"color\", \"name\", \"finish_time\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def plot_marble_race(players, seed=None):\n",
    "    \"\"\"'Simulate' and plot a marble race\"\"\"\n",
    "    if isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "    race_df = _gen_race_data(players)\n",
    "\n",
    "    color_df = race_df[[\"color\", \"name\"]].drop_duplicates()\n",
    "    color_discrete_map = {}\n",
    "    for _, row in color_df.iterrows():\n",
    "        color_discrete_map[row[\"name\"]] = row[\"color\"]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        data_frame=race_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"name\",\n",
    "        text=\"name\",\n",
    "        animation_frame=\"time\",\n",
    "        title=\"Thinkful Marble Racing Series\",\n",
    "        color_discrete_map=color_discrete_map,\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker={\"size\": 20})\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[1, 1], y=[-300, 300], mode=\"lines\", line={\"color\": \"black\"},)\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        {\"range\": [-0.1, 1.1], \"showgrid\": False, \"zeroline\": False, \"visible\": False,}\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        {\"range\": [-0.1, 1.1], \"showgrid\": False, \"zeroline\": False, \"visible\": False,}\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # To get consistent results between\n",
    "    # animation and score board.\n",
    "    # No editing needed.\n",
    "seed = np.random.randint(0, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Edit this to have the names of who/what is racing.\n",
    "racers = [\"A\", \"B\", \"C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Play out race visually.\n",
    "plot_marble_race(racers, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Show race results.\n",
    "    # Spoiler alert if this cell is run before plot_marble_race().\n",
    "marble_race(racers, seed).sort_values(\"finish_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                              class_notebooks/week3/confidence_intervals.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Sorry for breaking the immersion in the story line\n",
    "    # Just making up data.... next cell to attain statistics glory\n",
    "np.random.seed(42)\n",
    "\n",
    "og = pd.DataFrame({\"sales\": np.random.normal(200.0, 3, 1000), \"group\": \"old\"})\n",
    "new = pd.DataFrame({\"sales\": np.random.normal(200.5, 3, 1000), \"group\": \"new\"})\n",
    "\n",
    "sales = pd.concat((og, new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sales.shape\n",
    "sales.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.violinplot(x=\"group\", y=\"sales\", data=sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sns.boxplot(x=\"group\", y=\"sales\", data=sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "new = sales[sales[\"group\"] == \"new\"]\n",
    "old = sales[sales[\"group\"] == \"old\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.hist(old[\"sales\"], alpha=0.5, label=\"old\")\n",
    "plt.hist(new[\"sales\"], alpha=0.5, label=\"new\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.distplot(old[\"sales\"])\n",
    "plt.title(\"old\")\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(new[\"sales\"])\n",
    "plt.title(\"new\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "new = sales[sales[\"group\"] == \"new\"]\n",
    "old = sales[sales[\"group\"] == \"old\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(new[\"sales\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(old[\"sales\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t, p = stats.ttest_ind(new[\"sales\"], old[\"sales\"])\n",
    "p < 0.05\n",
    "    # Reject the null hypothesis.\n",
    "    # We see evidence that the new website will increase sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 = new[\"sales\"]\n",
    "x2 = old[\"sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signal = x1.mean() - x2.mean()\n",
    "noise = np.sqrt(x1.var() / x1.size + x2.var() / x2.size)\n",
    "\n",
    "ci_lo = signal - 1.96 * noise\n",
    "ci_hi = signal + 1.96 * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ci_lo, ci_hi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # It depends.  Is an extra $0.40 - $0.90 per sale a big difference in\n",
    "    # context? If yes, lets pull the trigger on the new site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # we only care about 2 species according to the prompt\n",
    "iris = iris[iris[\"species\"].isin([\"setosa\", \"versicolor\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # This plot is looking like theres a difference\n",
    "sns.swarmplot(\"species\", \"sepal_length\", data=iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "setosa = iris[iris[\"species\"] == \"setosa\"]\n",
    "versicolor = iris[iris[\"species\"] == \"versicolor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(setosa[\"sepal_length\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.describe(versicolor[\"sepal_length\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # For a qqplot, if the data is falling on the line, we're normal\n",
    "sm.qqplot(setosa[\"sepal_length\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sm.qqplot(versicolor[\"sepal_length\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "t, p = stats.ttest_ind(setosa[\"sepal_length\"], versicolor[\"sepal_length\"])\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"significant difference; reject null\")\n",
    "else:\n",
    "    print(\"insignificant difference; fail to reject null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_95_ci(x1, x2):\n",
    "    signal = x1.mean() - x2.mean()\n",
    "    noise = np.sqrt(x1.var() / len(x1) + x2.var() / len(x2))\n",
    "\n",
    "    ci_lo = signal - 1.96 * noise\n",
    "    ci_hi = signal + 1.96 * noise\n",
    "\n",
    "    return (ci_lo, ci_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_95_ci(setosa[\"sepal_length\"], versicolor[\"sepal_length\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # According to a 95% confidence interval, the versicolor sepal length\n",
    "    # is anywhere from 0.76 to 1.1 units greater than setosa sepal length.\n",
    "\n",
    "    # ^not perfect stats language but gets the job done.  For perfectly\n",
    "    # sound stats language you might report this analysis as below.  Choose\n",
    "    # your language based on your audience.\n",
    "\n",
    "    # A t-test was performed to compare the mean sepal length between\n",
    "    # the versicolor and setosa species of iris; the t-test showed\n",
    "    # a significant difference (p < 0.05). A 95% confidence interval\n",
    "    # for the difference between these means is [0.757, 1.103]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             class_notebooks/week3/marble_race.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _gen_player_data(color, name=\"\", y=0, n_steps=20):\n",
    "    \"\"\"Generate a random race time and split it out over time steps for plotting\"\"\"\n",
    "    # Generate random race times\n",
    "    finish_time = np.random.normal(n_steps * 0.8, 0.3)\n",
    "\n",
    "    # Find x position for each frame of race\n",
    "    rate = 1 / finish_time\n",
    "    step = finish_time / n_steps\n",
    "    time_steps = np.arange(n_steps + 1)\n",
    "    x_pos = time_steps * rate\n",
    "\n",
    "    # Store all plotting info for plotly\n",
    "    race_df = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": time_steps,\n",
    "            \"x\": x_pos,\n",
    "            \"y\": y,\n",
    "            \"color\": color,\n",
    "            \"name\": name,\n",
    "            \"finish_time\": finish_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add a little jitter to be less boring\n",
    "    excitement = np.ones_like(x_pos) * 0.01\n",
    "    excitement[: len(excitement) // 2] *= -1\n",
    "    np.random.shuffle(excitement)\n",
    "    race_df[\"x\"] += excitement\n",
    "    race_df.loc[0, \"x\"] = 0\n",
    "\n",
    "    return race_df\n",
    "\n",
    "\n",
    "def _gen_race_data(players, colors=px.colors.qualitative.T10):\n",
    "    \"\"\"'Simulate' a marble race between players\"\"\"\n",
    "    race_dfs = []\n",
    "    name_colors = zip(players, colors)\n",
    "    for i, (name, color) in enumerate(name_colors):\n",
    "        race_df = _gen_player_data(color, name, i * 0.1)\n",
    "        race_dfs.append(race_df)\n",
    "\n",
    "    return pd.concat(race_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def marble_race(players, seed=None):\n",
    "    \"\"\"'Simulate' a marble race\"\"\"\n",
    "    if isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "    race_df = _gen_race_data(players)\n",
    "\n",
    "    return (\n",
    "        race_df[[\"color\", \"name\", \"finish_time\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_marble_race(players, seed=None):\n",
    "    \"\"\"'Simulate' and plot a marble race\"\"\"\n",
    "    if isinstance(seed, int):\n",
    "        np.random.seed(seed)\n",
    "    race_df = _gen_race_data(players)\n",
    "\n",
    "    color_df = race_df[[\"color\", \"name\"]].drop_duplicates()\n",
    "    color_discrete_map = {}\n",
    "    for _, row in color_df.iterrows():\n",
    "        color_discrete_map[row[\"name\"]] = row[\"color\"]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        data_frame=race_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"name\",\n",
    "        text=\"name\",\n",
    "        animation_frame=\"time\",\n",
    "        title=\"Thinkful Marble Racing Series\",\n",
    "        color_discrete_map=color_discrete_map,\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker={\"size\": 20})\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[1, 1], y=[-300, 300], mode=\"lines\", line={\"color\": \"black\"},)\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        {\"range\": [-0.1, 1.1], \"showgrid\": False, \"zeroline\": False, \"visible\": False,}\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        {\"range\": [-0.1, 1.1], \"showgrid\": False, \"zeroline\": False, \"visible\": False,}\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # To get consistent results between\n",
    "    # animation and score board.\n",
    "    # No editing needed.\n",
    "seed = np.random.randint(0, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Edit this to have the names of who/what is racing.\n",
    "racers = [\"A\", \"B\", \"C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Play out race visually.\n",
    "plot_marble_race(racers, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Show race results.\n",
    "    # Spoiler alert if this cell is run before plot_marble_race().\n",
    "marble_race(racers, seed).sort_values(\"finish_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                               class_notebooks/week3/non_param_effect_size.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Independent samples -------\n",
    "# ---------------------------\n",
    "def cles_ind(x1, x2):\n",
    "    \"\"\"Calc common language effect size\n",
    "    Interpret as the probability that a score sampled\n",
    "    at random from one distribution will be greater than\n",
    "    a score sampled from some other distribution.\n",
    "    Based on: http://psycnet.apa.org/doi/10.1037/0033-2909.111.2.361\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) common language effect size\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diff = x1[:, None] - x2\n",
    "    cles = max((diff < 0).sum(), (diff > 0).sum()) / diff.size\n",
    "\n",
    "    return cles\n",
    "\n",
    "\n",
    "def rbc_ind(x1, x2):\n",
    "    \"\"\"Calculate rank-biserial correlation coefficient\n",
    "    Output values range from [0, 1]; interpret as:\n",
    "      * Values closer to 0 are a weaker effect\n",
    "      * Values closer to 1 are a stronger effect\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) rank-biserial correlation coefficient\n",
    "    \"\"\"\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "\n",
    "    u, _ = stats.mannwhitneyu(x1, x2)\n",
    "    rbc = 1 - (2 * u) / (n1 * n2)\n",
    "\n",
    "    return rbc\n",
    "\n",
    "\n",
    "def calc_non_param_ci(x1, x2, alpha=0.05):\n",
    "    \"\"\"Calc confidence interval for 2 group median test\n",
    "    Process:\n",
    "      * Find all pairwise diffs\n",
    "      * Sort diffs\n",
    "      * Find appropriate value of k\n",
    "      * Choose lower bound from diffs as: diffs[k]\n",
    "      * Choose upper bound from diffs as: diffs[-k]\n",
    "    Based on: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545906/\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :param alpha: significance level\n",
    "    :return: (tuple) confidence interval bounds\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "    cv = stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "    # Find pairwise differences for every datapoint in each group\n",
    "    diffs = (x1[:, None] - x2).flatten()\n",
    "    diffs.sort()\n",
    "\n",
    "    # For an approximate (1-a)% confidence interval first calculate K:\n",
    "    k = int(round(n1 * n2 / 2 - (cv * (n1 * n2 * (n1 + n2 + 1) / 12) ** 0.5)))\n",
    "\n",
    "    # The Kth smallest to the Kth largest of the n x m differences\n",
    "    # n1 and n2 should be > ~20\n",
    "    ci_lo = diffs[k]\n",
    "    ci_hi = diffs[-k]\n",
    "\n",
    "    return ci_lo, ci_hi\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Paired samples ------------\n",
    "# ---------------------------\n",
    "def cles_rel(x1, x2):\n",
    "    \"\"\"Calc common language effect size for paired samples\n",
    "    Interpret as the probability that a pair's difference (x1 - x2)\n",
    "    sampled at random will be greater than 0.\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) common language effect size\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diffs = x1 - x2\n",
    "    # Convert differences to 0.0, 0.5, or 1.0:\n",
    "    #   * 0.0 if x1 < x2\n",
    "    #   * 0.5 if x1 == x2\n",
    "    #   * 1.0 if x1 > x2\n",
    "    diffs = np.where(diffs == 0.0, 0.5, diffs > 0)\n",
    "\n",
    "    # Take average of array with [0s, 0.5s, 1s]\n",
    "    # This indicates prob of pulling a random\n",
    "    # diff and it being greater than 0\n",
    "    return diffs.mean()\n",
    "\n",
    "\n",
    "def rbc_rel(x1, x2):\n",
    "    \"\"\"Calculate rank-biserial correlation coefficient for paired samples\n",
    "    Output values range from [-1, 1]; interpret as:\n",
    "      * Values closer to 1 indicate that x1 is larger\n",
    "      * Values closer to -1 indicate that x2 is larger\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) rank-biserial correlation coefficient\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diffs = x1 - x2\n",
    "    diffs = diffs[diffs != 0]\n",
    "    diff_ranks = stats.rankdata(abs(diffs))\n",
    "\n",
    "    rank_sum = diff_ranks.sum()\n",
    "    pos_rank_sum = np.sum((diffs > 0) * diff_ranks)\n",
    "    neg_rank_sum = np.sum((diffs < 0) * diff_ranks)\n",
    "    rbc = pos_rank_sum / rank_sum - neg_rank_sum / rank_sum\n",
    "\n",
    "    return rbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                       class_notebooks/week3/non_parametric.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "    # Read kickstarter data\n",
    "postgres_user = \"dsbc_student\"\n",
    "postgres_pw = \"7*.8G9QH21\"\n",
    "postgres_host = \"142.93.121.174\"\n",
    "postgres_port = \"5432\"\n",
    "postgres_db = \"kickstarterprojects\"\n",
    "\n",
    "conn_str = f\"postgresql://{postgres_user}:{postgres_pw}@{postgres_host}:{postgres_port}/{postgres_db}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        pledged\n",
    "    FROM \n",
    "        ksprojects\n",
    "    WHERE category IN ('Performances', 'Theater', 'Music')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ks = pd.read_sql_query(query, conn_str)\n",
    "ks.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ks[\"category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ks.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.violinplot(\"category\", \"pledged\", data=ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # SELECT *\n",
    "    # FROM ks\n",
    "    # WHERE category = 'Music'\n",
    "music = ks[ks[\"category\"] == \"Music\"]\n",
    "thtr = ks[ks[\"category\"] == \"Theater\"]\n",
    "perf = ks[ks[\"category\"] == \"Performances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(music[\"pledged\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(thtr[\"pledged\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qqplot(perf[\"pledged\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # in general, to make it bigger use figure\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "\n",
    "ks.hist(by=\"category\", figsize=(10, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_, p = stats.kruskal(thtr[\"pledged\"], music[\"pledged\"], perf[\"pledged\"])\n",
    "p < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_, p1 = stats.mannwhitneyu(thtr[\"pledged\"], music[\"pledged\"])\n",
    "_, p2 = stats.mannwhitneyu(thtr[\"pledged\"], perf[\"pledged\"])\n",
    "_, p3 = stats.mannwhitneyu(music[\"pledged\"], perf[\"pledged\"])\n",
    "\n",
    "p_values = [p1, p2, p3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reject, corr_p, sidak, bonf = multipletests(p_values, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corr_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sidak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bonf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_non_param_ci(x1, x2, alpha=0.05):\n",
    "    \"\"\"Calc confidence interval for 2 group median test\n",
    "\n",
    "    Process:\n",
    "      * Find all pairwise diffs\n",
    "      * Sort diffs\n",
    "      * Find appropriate value of k\n",
    "      * Choose lower bound from diffs as: diffs[k]\n",
    "      * Choose upper bound from diffs as: diffs[-k]\n",
    "\n",
    "    Based on: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545906/\n",
    "\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :param alpha: significance level\n",
    "    :return: (tuple) confidence interval bounds\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "    cv = stats.norm.ppf(1 - alpha / 2)\n",
    "\n",
    "    # Find pairwise differences for every datapoint in each group\n",
    "    diffs = (x1[:, None] - x2).flatten()\n",
    "    diffs.sort()\n",
    "\n",
    "    # For an approximate (1-a)% confidence interval first calculate K:\n",
    "    k = int(round(n1 * n2 / 2 - (cv * (n1 * n2 * (n1 + n2 + 1) / 12) ** 0.5)))\n",
    "\n",
    "    # The Kth smallest to the Kth largest of the n x m differences\n",
    "    # n1 and n2 should be > ~20\n",
    "    ci_lo = diffs[k]\n",
    "    ci_hi = diffs[-k]\n",
    "\n",
    "    return ci_lo, ci_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(thtr[\"pledged\"].median())\n",
    "print(music[\"pledged\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_non_param_ci(thtr[\"pledged\"], music[\"pledged\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cles_ind(x1, x2):\n",
    "    \"\"\"Calc common language effect size\n",
    "\n",
    "    Interpret as the probability that a score sampled\n",
    "    at random from one distribution will be greater than\n",
    "    a score sampled from some other distribution.\n",
    "\n",
    "    Based on: http://psycnet.apa.org/doi/10.1037/0033-2909.111.2.361\n",
    "\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) common language effect size\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diff = x1[:, None] - x2\n",
    "    cles = max((diff < 0).sum(), (diff > 0).sum()) / diff.size\n",
    "\n",
    "    return cles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cles_ind(thtr[\"pledged\"], music[\"pledged\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def rbc_ind(x1, x2):\n",
    "    \"\"\"Calculate rank-biserial correlation coefficient\n",
    "\n",
    "    Output values range from [0, 1]; interpret as:\n",
    "      * Values closer to 0 are a weaker effect\n",
    "      * Values closer to 1 are a stronger effect\n",
    "\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) rank-biserial correlation coefficient\n",
    "    \"\"\"\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "\n",
    "    u, _ = stats.mannwhitneyu(x1, x2)\n",
    "    rbc = 1 - (2 * u) / (n1 * n2)\n",
    "\n",
    "    return rbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbc_ind(thtr[\"pledged\"], music[\"pledged\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # reformatting data for new setup\n",
    "n = min(thtr[\"pledged\"].size, music[\"pledged\"].size)\n",
    "\n",
    "t1 = thtr[\"pledged\"].values[:n]\n",
    "t1.sort()\n",
    "\n",
    "t2 = music[\"pledged\"].values[:n]\n",
    "t2.sort()\n",
    "\n",
    "df = pd.DataFrame({\"theater\": t1, \"music\": t2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"diff\"] = df[\"theater\"] - df[\"music\"]\n",
    "\n",
    "plt.hist(df[\"diff\"])\n",
    "plt.show()\n",
    "\n",
    "qqplot(df[\"diff\"], line=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats.wilcoxon(df[\"theater\"], df[\"music\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cles_rel(x1, x2):\n",
    "    \"\"\"Calc common language effect size for paired samples\n",
    "\n",
    "    Interpret as the probability that a pair's difference (x1 - x2)\n",
    "    sampled at random will be greater than 0.\n",
    "\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) common language effect size\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diffs = x1 - x2\n",
    "    # Convert differences to 0.0, 0.5, or 1.0:\n",
    "    #   * 0.0 if x1 < x2\n",
    "    #   * 0.5 if x1 == x2\n",
    "    #   * 1.0 if x1 > x2\n",
    "    diffs = np.where(diffs == 0.0, 0.5, diffs > 0)\n",
    "\n",
    "    # Take average of array with [0s, 0.5s, 1s]\n",
    "    # This indicates prob of pulling a random\n",
    "    # diff and it being greater than 0\n",
    "    return diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cles_rel(df[\"theater\"], df[\"music\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def rbc_rel(x1, x2):\n",
    "    \"\"\"Calculate rank-biserial correlation coefficient for paired samples\n",
    "\n",
    "    Output values range from [-1, 1]; interpret as:\n",
    "      * Values closer to 1 indicate that x1 is larger\n",
    "      * Values closer to -1 indicate that x2 is larger\n",
    "\n",
    "    :param x1: sample 1\n",
    "    :param x2: sample 2\n",
    "    :return: (float) rank-biserial correlation coefficient\n",
    "    \"\"\"\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "\n",
    "    diffs = x1 - x2\n",
    "    diffs = diffs[diffs != 0]\n",
    "    diff_ranks = stats.rankdata(abs(diffs))\n",
    "\n",
    "    rank_sum = diff_ranks.sum()\n",
    "    pos_rank_sum = np.sum((diffs > 0) * diff_ranks)\n",
    "    neg_rank_sum = np.sum((diffs < 0) * diff_ranks)\n",
    "    rbc = pos_rank_sum / rank_sum - neg_rank_sum / rank_sum\n",
    "\n",
    "    return rbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbc_rel(df[\"theater\"], df[\"music\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                          class_notebooks/week3/simpson_power_phack.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%reload_ext nb_black\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/kidney_stone_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "kidney = pd.read_csv(data_url)\n",
    "kidney.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kidney.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kidney.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kidney.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kidney[\"treatment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kidney[\"stone_size\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.crosstab(kidney[\"treatment\"], kidney[\"stone_size\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.crosstab(kidney[\"treatment\"], kidney[\"success\"], normalize=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Input your crosstab here w/o normalizing or row/col totals\n",
    "crosstab = pd.crosstab(kidney[\"treatment\"], kidney[\"success\"])\n",
    "chi2, p, df, expected = stats.chi2_contingency(crosstab)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.crosstab(\n",
    "    [kidney[\"treatment\"], kidney[\"stone_size\"]], kidney[\"success\"], normalize=\"index\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Input your crosstab here w/o normalizing or row/col totals\n",
    "crosstab = pd.crosstab([kidney[\"treatment\"], kidney[\"stone_size\"]], kidney[\"success\"])\n",
    "chi2, p, df, expected = stats.chi2_contingency(crosstab)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_95_ci(x1, x2):\n",
    "    \"\"\"Calculate a 95% CI for 2 1d numpy arrays\"\"\"\n",
    "    signal = x1.mean() - x2.mean()\n",
    "    noise = np.sqrt(x1.var() / x1.size + x2.var() / x2.size)\n",
    "\n",
    "    ci_lo = signal - 1.96 * noise\n",
    "    ci_hi = signal + 1.96 * noise\n",
    "\n",
    "    return ci_lo, ci_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Below package is needed for pandas to read xl files\n",
    "    # !pip install xlrd\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/a-b-testing-drill-start-06-14-19.xlsx\"\n",
    "\n",
    "df = pd.read_excel(data_url)\n",
    "\n",
    "treatment = df[df[\"group\"] == \"treatment\"]\n",
    "control = df[df[\"group\"] == \"control\"]\n",
    "\n",
    "    # The tetimon\n",
    "t, p = stats.ttest_ind(treatment[\"cart_amount\"], control[\"cart_amount\"])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_95_ci(treatment[\"cart_amount\"], control[\"cart_amount\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 = treatment[\"cart_amount\"]\n",
    "x2 = control[\"cart_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x2.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s_pooled_numerator = (x1.size - 1) * x1.var() + (x2.size - 1) * x2.var()\n",
    "s_pooled_denominator = x1.size + x2.size - 2\n",
    "\n",
    "s_pooled = np.sqrt(s_pooled_numerator / s_pooled_denominator)\n",
    "s_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # 95% CI was: (0.0986, 0.5416)\n",
    "effect_size = (x1.mean() - x2.mean()) / s_pooled\n",
    "effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # If asked about \n",
    "    # \"how much more money will we make per quarter with this?\"\n",
    "    # Assuming in a quarter we have 10000 customers\n",
    "\n",
    "    # Using CI\n",
    "10000 * 0.0986\n",
    "10000 * 0.5416\n",
    "\n",
    "    # Using effect size \n",
    "10000 * 0.1353"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis = TTestIndPower()\n",
    "\n",
    "    # Type 1 Error - False positive (reject null when no effect)\n",
    "    # Type 2 Error - False negative (fail to reject null when there is an effect)\n",
    "\n",
    "analysis.solve_power(\n",
    "    effect_size=effect_size, nobs1=x1.size, alpha=0.05, ratio=x1.size / x2.size\n",
    ")\n",
    "\n",
    "    # 15% chance of type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Renaming just to reuse without a lot of extra typing\n",
    "n = x1.size\n",
    "es = effect_size\n",
    "\n",
    "    # Making up potential sample/effect sizes that relate to what we observe\n",
    "sample_sizes = np.array([n * 0.1, n * 0.5, n, n * 2, n * 10])\n",
    "effect_sizes = np.array([es * 0.1, es * 0.5, es, es * 2])\n",
    "\n",
    "    # Plot the power if we had these sample/effect sizes\n",
    "analysis.plot_power(\n",
    "    dep_var=\"nobs\", nobs=sample_sizes, alpha=0.05, effect_size=effect_sizes\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_x1 = 11\n",
    "mean_x2 = 10\n",
    "std_x1 = 2\n",
    "std_x2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ns = [10, 50, 100, 500, 1000, 5000]\n",
    "ps = []\n",
    "for n in ns:\n",
    "    signal = mean_x1 - mean_x2\n",
    "    noise = np.sqrt(std_x1 ** 2 / n + std_x2 ** 2 / n)\n",
    "    t = signal / noise\n",
    "\n",
    "    # Look up p value for given value of t and sample size\n",
    "    p = stats.t.sf(np.abs(t), 2 * n - 2) * 2\n",
    "    ps.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ns = [10, 50, 100, 500, 1000, 5000]\n",
    "cis = []\n",
    "for i, n in enumerate(ns):\n",
    "    signal = mean_x1 - mean_x2\n",
    "    noise = np.sqrt((std_x1 ** 2 / n) + (std_x2 ** 2 / n))\n",
    "\n",
    "    ci_lo = signal - 1.96 * noise\n",
    "    ci_hi = signal + 1.96 * noise\n",
    "\n",
    "    ci = (ci_lo, ci_hi)\n",
    "    cis.append(ci)\n",
    "\n",
    "    color = plt.cm.RdYlBu(i * 60)\n",
    "    plt.hlines(len(ns) - i, ci_lo, ci_hi, colors=[color], label=f\"n={n}\")\n",
    "\n",
    "print(cis)\n",
    "\n",
    "plt.yticks([])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                 class_notebooks/week3/t_test_marbles.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
