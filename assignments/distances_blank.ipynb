{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "* Which supervised learning method's loss function is shown below? ($SSE$ is Sum of Squared Errors & $\\beta_i$ is the $i$th coefficient of the model)\n",
    "    * (A) Logisitic Regression\n",
    "    * (B) LASSO Regression\n",
    "    * (C) Ridge Regression\n",
    "    * (D) ElasticNet Regression\n",
    "\n",
    "$$SSE + \\lambda \\sum_{i=1}|\\beta_i|$$\n",
    "\n",
    "* Bonus for above, this method can also be referenced as \"\\_\\_\\_\\_\\_ Regularization\"\n",
    "    * (A) L1\n",
    "    * (B) L2\n",
    "    * (C) A1\n",
    "    * (D) B4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "* Which supervised learning method's loss function is shown below? ($SSE$ is Sum of Squared Errors & $\\beta_i$ is the $i$th coefficient of the model)\n",
    "    * (A) Logisitic Regression\n",
    "    * (B) LASSO Regression\n",
    "    * (C) Ridge Regression\n",
    "    * (D) ElasticNet Regression\n",
    "\n",
    "$$SSE + \\lambda \\sum_{i=1}\\beta_i^2$$\n",
    "\n",
    "* Bonus for above, this method can also be referenced as \"\\_\\_\\_\\_\\_ Regularization\"\n",
    "    * (A) L1\n",
    "    * (B) L2\n",
    "    * (C) A1\n",
    "    * (D) B4\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Some code for the remaining questions:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_test = np.array([ 6, -2, -4, 6,  -7])\n",
    "y_pred = np.array([ 4,  4, -3, 9, -30])\n",
    "\n",
    "mae = np.____(np.____(y_pred - y_test))\n",
    "rmse = np.____(np.____((y_pred - y_test) ** 2))\n",
    "```\n",
    "\n",
    "* What is the Mean Absolute Error in this case?\n",
    "\n",
    "* What is the Root Mean Squared Error in this case?\n",
    "\n",
    "\n",
    "* What are the differences between MAE and RMSE in this case?\n",
    "* What are the differences between the 2 in general?\n",
    "* Why do we have multiple metrics for error?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We're talking about distances today as if they're something new, but keep in mind you've been doing some distance calculations already: \n",
    "* Sometimes more explicitly - like when checking model performance by calculating MAE/RMSE (average distance between `y_pred` & `y_test`)\n",
    "* Sometimes less explicitly - like when using LASSO/Ridge/ElasticNet (distance between coefficients and 0)\n",
    "* Sometimes it was the whole point of the method - KNN (distance between observation and its nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances for continuous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Resource that has plans to include all the distances we'll cover: https://adamspannbauer.github.io/distance_metrics_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [[1, 30, 0, 3, 80000], [2, 32, 0, 4, 77000], [3, 55, 3, 12, 81000]],\n",
    "    columns=[\"id\", \"age\", \"n_children\", \"education\", \"income\"],\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "* Who are the most similar intuitively?\n",
    "* Calculate the distance between each row to support/refute your intuition.\n",
    "    * We'll calculate these first few 'by hand' and then use an imported function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "A more practical solution is use the `pdist` function from `scipy.spatial.distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Its often paired with the `squareform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "For a prettier print in jupyter you can convert to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "* What is the default method used in `pdist`?\n",
    "* What other distance methods does `pdist` provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "An example to show why cosine distance makes sense.\n",
    "\n",
    "Let's say the below data is some features made from 2 different blogs.  The way we created this data is by counting the number of times each word appeared in each blog.\n",
    "\n",
    "One was a political blog talking about Donald Trump; the other was a game blog talking about the intricacies of a card game's 'trump card' mechanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame({\"trump\": [20, 40], \"card\": [1, 45], \"donald\": [17, 0]})\n",
    "text_df.index = [\"blog1\", \"blog2\"]\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The option to visualize that we've been using so far together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(text_df[\"trump\"][0], text_df[\"card\"][0], color=\"blue\", label=\"blog1\")\n",
    "plt.scatter(text_df[\"trump\"][1], text_df[\"card\"][1], color=\"orange\", label=\"blog2\")\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"n(trump)\")\n",
    "plt.ylabel(\"n(card)\")\n",
    "plt.xlim(-1, 48)\n",
    "plt.ylim(-1, 48)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Another way to visualize/think about this is the data as vectors.  The beginnings of the vectors originate from the origin, and the tips of the vectors point to the location of our data as shown in the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "plt.quiver(\n",
    "    [0], [0],\n",
    "    text_df[\"trump\"][0], text_df[\"card\"][0],\n",
    "    color=\"blue\", label=\"blog1\",\n",
    "    angles=\"xy\", scale_units=\"xy\", scale=1,\n",
    ")\n",
    "plt.quiver(\n",
    "    [0], [0],\n",
    "    text_df[\"trump\"][1], text_df[\"card\"][1],\n",
    "    color=\"orange\", label=\"blog2\",\n",
    "    angles=\"xy\", scale_units=\"xy\", scale=1,\n",
    ")\n",
    "# fmt: on\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"n(trump)\")\n",
    "plt.ylabel(\"n(card)\")\n",
    "plt.xlim(-1, 48)\n",
    "plt.ylim(-1, 48)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We now have the same style of data but for a shorter post.  Which blog is the post most similar to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "new_observation = pd.DataFrame(\n",
    "    {\"trump\": [5], \"card\": [6], \"donald\": [0]}, index=[\"new_post\"]\n",
    ")\n",
    "\n",
    "full_df = pd.concat((text_df, new_observation))\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot makes our minds think more in terms of euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(full_df[\"trump\"][0], full_df[\"card\"][0], color=\"blue\", label=\"blog1\")\n",
    "plt.scatter(full_df[\"trump\"][1], full_df[\"card\"][1], color=\"orange\", label=\"blog2\")\n",
    "plt.scatter(\n",
    "    full_df[\"trump\"][2], full_df[\"card\"][2], color=\"black\", label=\"new_post\",\n",
    ")\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"n(trump)\")\n",
    "plt.ylabel(\"n(card)\")\n",
    "plt.xlim(-1, 48)\n",
    "plt.ylim(-1, 48)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual with a more vector representation of this data tells a different story of similarity/distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "plt.quiver(\n",
    "    full_df[\"trump\"][0], full_df[\"card\"][0],\n",
    "    color=\"blue\", label=\"blog1\",\n",
    "    angles=\"xy\", scale_units=\"xy\", scale=1,\n",
    ")\n",
    "plt.quiver(\n",
    "    full_df[\"trump\"][1], full_df[\"card\"][1],\n",
    "    color=\"orange\", label=\"blog2\",\n",
    "    angles=\"xy\", scale_units=\"xy\", scale=1,\n",
    ")\n",
    "plt.quiver(\n",
    "    full_df[\"trump\"][2], full_df[\"card\"][2],\n",
    "    color=\"black\", label=\"new_post\",\n",
    "    angles=\"xy\", scale_units=\"xy\", scale=1,\n",
    ")\n",
    "# fmt: on\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"n(trump)\")\n",
    "plt.ylabel(\"n(card)\")\n",
    "plt.xlim(-1, 48)\n",
    "plt.ylim(-1, 48)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "euclid_dist = squareform(pdist(full_df))\n",
    "cosine_dist = squareform(pdist(full_df, metric=\"cosine\"))\n",
    "\n",
    "euclid_dist = pd.DataFrame(euclid_dist, columns=full_df.index, index=full_df.index)\n",
    "cosine_dist = pd.DataFrame(cosine_dist, columns=full_df.index, index=full_df.index)\n",
    "\n",
    "print(\"Original Data\")\n",
    "display(full_df)\n",
    "\n",
    "print(\"\\nEuclidean Distance\")\n",
    "display(euclid_dist)\n",
    "\n",
    "print(\"\\n1 - Cosine Similarity\")\n",
    "display(cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist([[2, 3], [4, 6]], metric=\"cityblock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric distance cheat sheet:\n",
    "\n",
    "#### Manhattan distance\n",
    "\n",
    "$$\\sum_{i=0}^n|x_i - y_i|$$\n",
    "\n",
    "* Intuition: \"Taxi cab/city block distance\" This metric will be less affected by outlier differences in the calculation than euclidean.\n",
    "* Examples:\n",
    "    * `manhattan([0,0], [3,4])` is 7\n",
    "    * `manhattan([0,0], [3,10])` is 13\n",
    "    * `manhattan([2,3], [4,6])` is 5\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_1$ norm\" (is minkowski distance ($L_p$) with $p=1$)\n",
    "\n",
    "\n",
    "#### Euclidean distance\n",
    "\n",
    "$$\\sqrt{\\sum_{i=0}^n(x_i - y_i)^2}$$\n",
    "\n",
    "* Intuition: \"Straight line distance.\" This metric will be more affected by outlier differences in the calculation than Manahattan (due to being squared).\n",
    "* Examples:\n",
    "    * `euclidean([0,0], [3,4])` is 5\n",
    "    * `euclidean([0,0], [3,10])` is 10.44\n",
    "    * `euclidean([2,3], [4,6])` is 3.606\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_2$ norm\" (is minkowski distance ($L_p$) with $p=2$)\n",
    "\n",
    "\n",
    "#### Chebyshev distance\n",
    "\n",
    "$$max(|x_i - y_i|)$$\n",
    "\n",
    "* Intuition: \"The biggest difference between the 2 rows.\" This metric is only affected by outlier differences in the calculation.  (it's only the max)\n",
    "* Examples:\n",
    "    * `chebyshev([0,0], [3,4])` is 4\n",
    "    * `chebyshev([0,0], [3,10])` is 10\n",
    "    * `chebyshev([2,3], [4,6])` is 3\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_\\infty$ norm\" (is minkowski distance ($L_p$) with $p=\\infty$)\n",
    "\n",
    "#### Cosine disimilarity\n",
    "\n",
    "$$max(|x_i - y_i|)$$\n",
    "\n",
    "* Intuition: \"Angle between the vectors defined by each observation.\"  Focuses more on how each column relates to each other within each observation; if they relationships between columns are the same then this is a small distance.\n",
    "* Examples:\n",
    "    * `cosine_dis([0,0], [3,4])` is nan\n",
    "    * `cosine_dis([0,0], [3,10])` is nan\n",
    "    * `cosine_dis([0,0,1], [3,10,1])` is 0.904\n",
    "    * `cosine_dis([2,3], [4,6])` is 0\n",
    "* Code: `pdist(x)` or `pdist(x, metric='euclidean')`\n",
    "* \"$L_\\infty$ norm\" (is minkowski distance ($L_p$) with $p=\\infty$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "### Scenario 1:\n",
    "\n",
    "Which users are the most similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"subscriber\": [\"yes\", \"no\", \"no\"],\n",
    "        \"dog_owner\": [\"yes\", \"yes\", \"no\"],\n",
    "        \"cat_owner\": [\"yes\", \"yes\", \"no\"],\n",
    "        \"smoker\": [\"yes\", \"yes\", \"yes\"],\n",
    "    },\n",
    "    index=[\"user_1\", \"user_2\", \"user_3\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the data for an ML model to consume it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does this encoding change how similar things look?\n",
    "* In regards to comparing rows:\n",
    "    * what does it mean when both rows have a `0`?\n",
    "    * what does it mean when both rows have a `1`?\n",
    "    * what does it mean when one row has a `0` and one row has a `1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2:\n",
    "\n",
    "Which users are most similiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"region\": [\"west\", \"south\", \"south\", \"north\", \"east\"],\n",
    "        \"favorite_show\": [\n",
    "            \"office\",\n",
    "            \"sportscenter\",\n",
    "            \"office\",\n",
    "            \"sportscenter\",\n",
    "            \"bachelor\",\n",
    "        ],\n",
    "        \"music_service\": [\"spotify\", \"apple\", \"spotify\", \"pandora\", \"apple\"],\n",
    "        \"favorite_planet\": [\"earth\", \"pluto\", \"earth\", \"pandora\", \"pluto\"],\n",
    "    },\n",
    "    index=[\"user_1\", \"user_2\", \"user_3\", \"user_4\", \"user_5\"],\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the data for an ML model to consume it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does this encoding change how similar things look?\n",
    "* In regards to comparing rows:\n",
    "    * what does it mean when both rows have a `0`?\n",
    "    * what does it mean when both rows have a `1`?\n",
    "    * what does it mean when one row has a `0` and one row has a `1`?\n",
    "    \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical distance cheat sheet:\n",
    "\n",
    "#### Hamming distance\n",
    "\n",
    "$$\\frac{n_{misses}}{n_{columns}}$$\n",
    "\n",
    "* **Makes a lot of sense for binary columns where a `0` is a meaningful response.**\n",
    "* Intuition: \"What fraction of the elements between the 2 rows are differnt?\"\n",
    "* Examples:\n",
    "    * `hamming([0,0,0], [1,1,1])` is $\\frac{3}{3}$ = 1\n",
    "    * `hamming([1,0,0], [1,1,1])` is $\\frac{2}{3}$\n",
    "    * `hamming([1,1,0], [1,1,1])` is $\\frac{1}{3}$\n",
    "    * `hamming([1,1,1], [1,1,1])` is $\\frac{0}{3}$ = 0\n",
    "    * `hamming([0,0,1], [0,0,0])` is $\\frac{1}{3}$\n",
    "    * `hamming([0,0,1], [0,1,1])` is $\\frac{1}{3}$\n",
    "* Code: `pdist(x, metric='hamming')` or `pdist(x, metric='matching')`\n",
    "\n",
    "\n",
    "#### Dice dissimilarity\n",
    "\n",
    "$$\\frac{n_{misses}}{2n_{one\\_matches} + n_{misses}}$$\n",
    "\n",
    "* **Makes a lot of sense for dummy columns where a `0` is a less meaningful response, but matching on a 1 means a lot (i.e. a dummy matching on 1 means the original input categorical data matched).**\n",
    "* Intuition: \"Hamming distance but... ignore matches of `0`s and extra count matches of `1`s\"\n",
    "* Examples:\n",
    "    * `dice([0,0,0], [1,1,1])` is $\\frac{3}{2(0) + 3}$ = 1\n",
    "    * `dice([1,0,0], [1,1,1])` is $\\frac{2}{2(1) + 2}$ = $\\frac{1}{2}$\n",
    "    * `dice([1,1,0], [1,1,1])` is $\\frac{1}{2(2) + 1}$ = $\\frac{1}{5}$\n",
    "    * `dice([1,1,1], [1,1,1])` is $\\frac{0}{2(3) + 0}$ = 0\n",
    "    * `dice([0,0,1], [0,0,0])` is $\\frac{1}{2(0) + 1}$ = 1\n",
    "    * `dice([0,0,1], [0,1,1])` is $\\frac{1}{2(1) + 1}$ = $\\frac{1}{3}$\n",
    "* Code: `pdist(x, metric='dice')`\n",
    "\n",
    "\n",
    "#### Jaccard distance\n",
    "\n",
    "$$\\frac{n_{misses}}{n_{one\\_matches} + n_{misses}}$$\n",
    "\n",
    "* **Makes a lot of sense for a mix of binary and dummy columns**\n",
    "* Intuition: \"What if there was a middle ground between hamming and dice?\"\n",
    "* Examples:\n",
    "    * `jaccard([0,0,0], [1,1,1])` is $\\frac{3}{0 + 3}$ = 1\n",
    "    * `jaccard([1,0,0], [1,1,1])` is $\\frac{2}{1 + 2}$ = $\\frac{2}{3}$\n",
    "    * `jaccard([1,1,0], [1,1,1])` is $\\frac{1}{2 + 1}$ = $\\frac{1}{3}$\n",
    "    * `jaccard([1,1,1], [1,1,1])` is $\\frac{0}{0 + 3}$ = 0\n",
    "    * `jaccard([0,0,1], [0,0,0])` is $\\frac{1}{0 + 1}$ = 1\n",
    "    * `jaccard([0,0,1], [0,1,1])` is $\\frac{1}{1 + 1}$ = $\\frac{1}{2}$\n",
    "* Code: `pdist(x, metric='jaccard')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
